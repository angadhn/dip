{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the classic cart-pole system implemented by Rich Sutton et al.\n",
    "\"\"\"\n",
    "\n",
    "class DoubleInvertedPendulumMA(gym.Env): \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.81\n",
    "        self.masscart = 1.0\n",
    "        self.masspole_1 = 0.1\n",
    "        self.masspole_2 = 0.1\n",
    "        self.masspole = (self.masspole_1 + self.masspole_2)\n",
    "        self.total_mass = (self.masscart + self.masspole)\n",
    "        self.lengthpole_1 = 0.25\n",
    "        self.lengthpole_2 = 0.25\n",
    "        self.length = (self.lengthpole_1 + self.lengthpole_2) \n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.phi_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        # distance of cart to fail episode\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.phi_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot, phi, phi_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        cosphi = math.cos(phi)\n",
    "        sinphi = math.cos(phi)\n",
    "        \n",
    "        # TODO: double-check the equations\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        \n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "\n",
    "        phiacc = (self.gravity * sinphi - sinphi * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * cosphi ** 2 / self.total_mass))\n",
    "        \n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi = phi + self.tau * phi_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            # print(f\"phi_dot: {phi_dot}, theta_dot{theta_dot}\")\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            phi = phi + self.tau * phi_dot  \n",
    "            \n",
    "        self.state = (x, x_dot, theta, theta_dot, phi, phi_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "            or phi < -self.phi_threshold_radians\n",
    "            or phi > self.phi_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(6,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 8.00 |\n",
      "| episode:  1  | score: 9.00 |\n",
      "| episode:  2  | score: 11.00 |\n",
      "| episode:  3  | score: 12.00 |\n",
      "| episode:  4  | score: 8.00 |\n",
      "| episode:  5  | score: 8.00 |\n",
      "| episode:  6  | score: 9.00 |\n",
      "| episode:  7  | score: 10.00 |\n",
      "| episode:  8  | score: 11.00 |\n",
      "| episode:  9  | score: 9.00 |\n",
      "| episode:  10  | score: 9.00 |\n",
      "| episode:  11  | score: 12.00 |\n",
      "| episode:  12  | score: 9.00 |\n",
      "| episode:  13  | score: 11.00 |\n",
      "| episode:  14  | score: 11.00 |\n",
      "| episode:  15  | score: 9.00 |\n",
      "| episode:  16  | score: 9.00 |\n",
      "| episode:  17  | score: 9.00 |\n",
      "| episode:  18  | score: 11.00 |\n",
      "| episode:  19  | score: 10.00 |\n",
      "| episode:  20  | score: 9.00 |\n",
      "| episode:  21  | score: 11.00 |\n",
      "| episode:  22  | score: 10.00 |\n",
      "| episode:  23  | score: 9.00 |\n",
      "| episode:  24  | score: 7.00 |\n",
      "| episode:  25  | score: 12.00 |\n",
      "| episode:  26  | score: 11.00 |\n",
      "| episode:  27  | score: 9.00 |\n",
      "| episode:  28  | score: 9.00 |\n",
      "| episode:  29  | score: 9.00 |\n",
      "| episode:  30  | score: 12.00 |\n",
      "| episode:  31  | score: 11.00 |\n",
      "| episode:  32  | score: 8.00 |\n",
      "| episode:  33  | score: 10.00 |\n",
      "| episode:  34  | score: 13.00 |\n",
      "| episode:  35  | score: 10.00 |\n",
      "| episode:  36  | score: 10.00 |\n",
      "| episode:  37  | score: 11.00 |\n",
      "| episode:  38  | score: 12.00 |\n",
      "| episode:  39  | score: 10.00 |\n",
      "| episode:  40  | score: 10.00 |\n",
      "| episode:  41  | score: 7.00 |\n",
      "| episode:  42  | score: 11.00 |\n",
      "| episode:  43  | score: 11.00 |\n",
      "| episode:  44  | score: 9.00 |\n",
      "| episode:  45  | score: 7.00 |\n",
      "| episode:  46  | score: 10.00 |\n",
      "| episode:  47  | score: 10.00 |\n",
      "| episode:  48  | score: 9.00 |\n",
      "| episode:  49  | score: 12.00 |\n",
      "| episode:  50  | score: 9.00 |\n",
      "| episode:  51  | score: 8.00 |\n",
      "| episode:  52  | score: 9.00 |\n",
      "| episode:  53  | score: 9.00 |\n",
      "| episode:  54  | score: 11.00 |\n",
      "| episode:  55  | score: 12.00 |\n",
      "| episode:  56  | score: 10.00 |\n",
      "| episode:  57  | score: 8.00 |\n",
      "| episode:  58  | score: 9.00 |\n",
      "| episode:  59  | score: 9.00 |\n",
      "| episode:  60  | score: 12.00 |\n",
      "| episode:  61  | score: 11.00 |\n",
      "| episode:  62  | score: 9.00 |\n",
      "| episode:  63  | score: 9.00 |\n",
      "| episode:  64  | score: 12.00 |\n",
      "| episode:  65  | score: 13.00 |\n",
      "| episode:  66  | score: 10.00 |\n",
      "| episode:  67  | score: 11.00 |\n",
      "| episode:  68  | score: 11.00 |\n",
      "| episode:  69  | score: 9.00 |\n",
      "| episode:  70  | score: 11.00 |\n",
      "| episode:  71  | score: 11.00 |\n",
      "| episode:  72  | score: 11.00 |\n",
      "| episode:  73  | score: 10.00 |\n",
      "| episode:  74  | score: 9.00 |\n",
      "| episode:  75  | score: 9.00 |\n",
      "| episode:  76  | score: 12.00 |\n",
      "| episode:  77  | score: 11.00 |\n",
      "| episode:  78  | score: 9.00 |\n",
      "| episode:  79  | score: 9.00 |\n",
      "| episode:  80  | score: 11.00 |\n",
      "| episode:  81  | score: 11.00 |\n",
      "| episode:  82  | score: 12.00 |\n",
      "| episode:  83  | score: 12.00 |\n",
      "| episode:  84  | score: 11.00 |\n",
      "| episode:  85  | score: 11.00 |\n",
      "| episode:  86  | score: 9.00 |\n",
      "| episode:  87  | score: 11.00 |\n",
      "| episode:  88  | score: 10.00 |\n",
      "| episode:  89  | score: 10.00 |\n",
      "| episode:  90  | score: 9.00 |\n",
      "| episode:  91  | score: 10.00 |\n",
      "| episode:  92  | score: 11.00 |\n",
      "| episode:  93  | score: 10.00 |\n",
      "| episode:  94  | score: 12.00 |\n",
      "| episode:  95  | score: 12.00 |\n",
      "| episode:  96  | score: 8.00 |\n",
      "| episode:  97  | score: 8.00 |\n",
      "| episode:  98  | score: 11.00 |\n",
      "| episode:  99  | score: 11.00 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = DoubleInvertedPendulumMA()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.2f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
