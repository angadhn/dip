{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Deep Reinforcement Learning for Robotic Systems </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the end-to-end mathematical and computational modelling of an **inverted double pendulum** with the integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm as the control system. This project serves as a baseline study into advanced autonomous control systems facilitating the control of multibody, variable mass dynamical systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-childhood",
   "metadata": {},
   "source": [
    "## Contents<a class=\"anchor\" id=\"contents\"></a>\n",
    "\n",
    "**1. [Overview](#overview)<br />**\n",
    "\n",
    "**2. [Model](#model)<br />**\n",
    "&emsp;2.1. [Description](#model-description)<br />\n",
    "\n",
    "**3. [Governing Equations of Motion](#governing-eqs-motion)<br />**\n",
    "&emsp;3.1. [Library Imports](#library-imports)<br />\n",
    "&emsp;3.2. [Variable Declaration](#var-dec)<br />\n",
    "&emsp;3.3. [Kinetic and Potential Energy](#kinetic-potential)<br />\n",
    "&emsp;3.4. [The Lagrangian](#lagrangian)<br />\n",
    "&emsp;3.5. [The Euler-Lagrange Equations](#euler-lagrange)<br />\n",
    "&emsp;3.6. [Linearisation and Acceleration](#linearisation)<br />\n",
    "\n",
    "**4. [Proximal Policy Optimisation](#ppo)**<br />\n",
    "&emsp;4.1. [Overview](#ppo-overview)<br />\n",
    "&emsp;4.2. [Mathematical Model](#ppo-math)<br />\n",
    "&emsp;4.3. [Neural Network](#ppo-nn)<br />\n",
    "&emsp;&emsp;&emsp;*4.3.1. [Actor](#ppo-nn-actor)<br />*\n",
    "&emsp;&emsp;&emsp;*4.3.2. [Critic](#ppo-nn-critic)<br />*\n",
    "&emsp;&emsp;&emsp;*4.3.3. [Agent](#ppo-nn-agent)<br />*\n",
    "&emsp;4.4. [Environment](#ppo-env)<br />\n",
    "&emsp;4.5. [Test](#ppo-test)<br />\n",
    "\n",
    "**5. [Conclusion](#conclusion)**<br />\n",
    "&emsp;5.1. [Variations in initial angle conditions](#conclusion-init-angles)<br />\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "* Press [☝](#contents) to return to the contents.\n",
    "* Prior to running the notebook, please [install](https://github.com/mughees-asif/dip#environment-setup) the following dependencies:\n",
    "  * `SymPy`\n",
    "  * `NumPy`\n",
    "  * `Matplotlib`\n",
    "  * `PyTorch`\n",
    "* Make sure to have the `seeding.py` [file](https://github.com/mughees-asif/dip) locally in the same folder as the notebook.\n",
    "* To simulate the system according to your parameters:\n",
    "  * Change the system parameters in the [Environment](#ppo-env) section.\n",
    "  * Change the number of games to be executed in the [Test](#ppo-test) section and run the same cell to get results.\n",
    "* Any problems, [email](mailto:mughees460@gmail.com) me or send me a ping on [LinkedIn](https://www.linkedin.com/in/mugheesasif/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview <a class=\"anchor\" id=\"overview\"></a>  [☝](#contents)\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-olympus",
   "metadata": {},
   "source": [
    "## 2. Model <a class=\"anchor\" id=\"model\"></a> [☝](#contents)\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" title=\"Inverted Double Pendulum\"/>\n",
    "\n",
    "| Name | Symbol |\n",
    "| :-: | :-: | \n",
    "| Mass of the cart | $$m$$ | \n",
    "| Mass of the pendulums | $$M_1 = M_2 = M$$ |\n",
    "| Length of the pendulums | $$l_1 = l_2 = l$$ |\n",
    "| Angle of the first pendulum <br/> w.r.t the vertical (CCW+) | $$\\theta$$ |\n",
    "| Angle of the second pendulum <br/> w.r.t the first pendulum (CCW+) | $$\\phi$$ |\n",
    "| Moment of inertia for the pendulums | $$I_1 = I_2 = I$$ |\n",
    "| Horizontal cart position | $$x$$ |\n",
    "| Horizontal force applied to the cart | $$u$$ |\n",
    "| Gravitational constant | $$g$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "### 2.1. Description <a class=\"anchor\" id=\"model-description\"></a> [☝](#contents)\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a holonomic, non-linear and chaotic mechanical system that relies on the *Butterfly Effect*: highly dependent on the initial conditions.\n",
    "\n",
    "#### Assumptions:\n",
    "* The mass, length & moment of inertia of the pendulums are equal.\n",
    "* No frictional forces exist between the surface and the cart wheels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 3. Governing Equations of Motion <a class=\"anchor\" id=\"governing-eqs-motion\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-portable",
   "metadata": {},
   "source": [
    "### 3.1. Library Imports <a class=\"anchor\" id=\"library-imports\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import gym\n",
    "import seeding \n",
    "import numpy as np\n",
    "import torch as T \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-brand",
   "metadata": {},
   "source": [
    "### 3.2. Variable Declaration <a class=\"anchor\" id=\"var-dec\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant\n",
    "u = sympy.symbols('u')        # force applied to the cart (horizontal component)\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of time `t`\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "Θ_dot = Θ.diff(t)               # | \n",
    "Θ_ddot = Θ_dot.diff(t)          # |\n",
    "Φ_dot = Φ.diff(t)               # |\n",
    "Φ_ddot = Φ_dot.diff(t)          # |\n",
    "cos_theta = sympy.cos(Θ)        # | --- experimental parameters\n",
    "sin_theta = sympy.sin(Θ)        # |\n",
    "cos_thetaphi = sympy.cos(Θ - Φ) # |\n",
    "cos_phi = sympy.cos(Φ)          # |\n",
    "sin_phi = sympy.sin(Φ)          # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 3.3. Kinetic (K.E.) and Potential (P.E.) Energy <a class=\"anchor\" id=\"kinetic-potential\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-american",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" title=\"Angular Trajectory of the Inverted Double Pendulum\"/>\n",
    "\n",
    "\\begin{equation*} \n",
    "\\because K.E._{T}= K.E._{C} + K.E._{1} + K.E._{2}\n",
    "\\label{eq:ke_4} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "\\because P.E._{T}= P.E._{C} + P.E._{1} + P.E._{2}\n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{equation*} \n",
    "K.E._{C}=\\frac{1}{2}m{\\dot{x}^2} \n",
    "\\label{eq:ke_1} \\tag{1} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "K.E._{1}=\\frac{1}{2}M\\left[\\dot{x}\\left(\\dot{x}+2l\\dot{\\theta{}}\\cos{\\theta{}}\\right)+{\\dot{\\theta{}}}^2\\left(Ml^2+I\\right)\\right] \n",
    "\\label{eq:ke_2} \\tag{2} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "K.E._{2}=\\frac{1}{2}\\left[{\\dot{x}}^2+l^2{\\dot{\\theta{}}}^2+{\\dot{\\phi{}}}^2\\left(Ml^2+I\\right)+Ml\\dot{\\theta{}}\\dot{\\phi{}}\\cos{\\left(\\theta{}-\\phi{}\\right)}+2Ml\\dot{x}\\left(\\dot{\\theta{}}\\cos{\\theta{}}+\\dot{\\phi{}}\\cos{\\phi{}}\\right)\\right] \n",
    "\\label{eq:ke_3} \\tag{3} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "\\therefore \\boldsymbol{K.E._{T}=\\frac{1}{2}\\left[m{\\dot{x}}^2+M\\dot{x}\\left(\\dot{x}+2l\\dot{\\theta{}}\\cos{\\theta{}}\\right)+{\\dot{\\theta{}}}^2\\left(Ml^2+I\\right)+{\\dot{x}}^2+l^2{\\dot{\\theta{}}}^2+\\\\{\\dot{\\phi{}}}^2\\left(Ml^2+I\\right)+Ml\\dot{\\theta{}}\\dot{\\phi{}}\\cos{\\left(\\theta{}-\\phi{}\\right)}+2Ml\\dot{x}\\left(\\dot{\\theta{}}\\cos{\\theta{}}+\\dot{\\phi{}}\\cos{\\phi{}}\\right)\\right]}\n",
    "\\label{eq:ke_5} \\tag{4} \n",
    "\\end{equation*}\n",
    "\n",
    "---\n",
    "\n",
    "\\begin{equation*} \n",
    "P.E._{C}=0\n",
    "\\tag{5} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "P.E._{1}=Mgl\\cos{\\theta}\n",
    "\\tag{6} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "P.E._{2}=Mgl\\cos{\\theta} + Mgl\\cos{\\phi}\n",
    "\\tag{7} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "\\therefore \\boldsymbol{P.E._{T}=Mgl(2\\cos{\\theta} + \\cos{\\phi})}\n",
    "\\tag{8} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy components\n",
    "# cart - linear\n",
    "k_1 = m*x_dot**2         \n",
    "\n",
    "# pendulum(s) - angular\n",
    "k_2 = (M*x_dot*(x_dot + 2*l*Θ_dot*cos_theta) + Θ_dot**2*(M*(l**2)+I))\n",
    "k_3 = (x_dot**2) + (l**2*Θ_dot**2) + (Φ_dot**2*(M*(l**2)+I) \\\n",
    "       + (M*l*Θ_dot*Φ_dot*cos_thetaphi) + \\\n",
    "       (2*M*l*x_dot*((Θ_dot*cos_theta) + (Φ_dot*cos_phi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "The kinetic energy, K, of the system:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.0 M l \\left(\\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Φ{\\left(t \\right)}\\right) \\frac{d}{d t} x{\\left(t \\right)} + 0.5 M l \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 0.5 M \\left(2 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + \\frac{d}{d t} x{\\left(t \\right)}\\right) \\frac{d}{d t} x{\\left(t \\right)} + 0.5 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2} + 0.5 \\left(I + M l^{2}\\right) \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 0.5 \\left(I + M l^{2}\\right) \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 0.5 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "1.0*M*l*(cos(Θ(t))*Derivative(Θ(t), t) + cos(Φ(t))*Derivative(Φ(t), t))*Derivative(x(t), t) + 0.5*M*l*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 0.5*M*(2*l*cos(Θ(t))*Derivative(Θ(t), t) + Derivative(x(t), t))*Derivative(x(t), t) + 0.5*l**2*Derivative(Θ(t), t)**2 + 0.5*m*Derivative(x(t), t)**2 + 0.5*(I + M*l**2)*Derivative(Θ(t), t)**2 + 0.5*(I + M*l**2)*Derivative(Φ(t), t)**2 + 0.5*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total kinetic energy\n",
    "K = 0.5*(k_1 + k_2 + k_3)\n",
    "\n",
    "print('----\\nThe kinetic energy, K, of the system:\\n----')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "becoming-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "The potential energy, P, of the system:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total potential energy\n",
    "P = M*g*l*((2*cos_theta) + cos_phi)\n",
    "\n",
    "print('----\\nThe potential energy, P, of the system:\\n----')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 3.4. The Lagrangian <a class=\"anchor\" id=\"lagrangian\"></a> [☝](#contents)\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$\\because S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "and since, $\\mathcal{L} = K - P,$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} \\mathcal{L} \\;dt$$\n",
    "\n",
    "where,\n",
    "\n",
    "\\begin{equation*} \n",
    "\\boldsymbol{\\mathcal{L}=\\frac{1}{2}\\left[m{\\dot{x}}^2+M\\dot{x}\\left(\\dot{x}+2l\\dot{\\theta{}}\\cos{\\theta{}}\\right)+{\\dot{\\theta{}}}^2\\left(Ml^2+I\\right)+{\\dot{x}}^2+l^2{\\dot{\\theta{}}}^2+\\\\{\\dot{\\phi{}}}^2\\left(Ml^2+I\\right)+Ml\\dot{\\theta{}}\\dot{\\phi{}}\\cos{\\left(\\theta{}-\\phi{}\\right)}+2Ml\\dot{x}\\left(\\dot{\\theta{}}\\cos{\\theta{}}+\\dot{\\phi{}}\\cos{\\phi{}}\\right)\\right] - Mgl(2\\cos{\\theta} + \\cos{\\phi})}\n",
    "\\tag{9} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "The Lagrangian of the system is:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 1.0 M l \\left(\\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Φ{\\left(t \\right)}\\right) \\frac{d}{d t} x{\\left(t \\right)} + 0.5 M l \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 0.5 M \\left(2 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + \\frac{d}{d t} x{\\left(t \\right)}\\right) \\frac{d}{d t} x{\\left(t \\right)} + 0.5 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2} + 0.5 \\left(I + M l^{2}\\right) \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 0.5 \\left(I + M l^{2}\\right) \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 0.5 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "-M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 1.0*M*l*(cos(Θ(t))*Derivative(Θ(t), t) + cos(Φ(t))*Derivative(Φ(t), t))*Derivative(x(t), t) + 0.5*M*l*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 0.5*M*(2*l*cos(Θ(t))*Derivative(Θ(t), t) + Derivative(x(t), t))*Derivative(x(t), t) + 0.5*l**2*Derivative(Θ(t), t)**2 + 0.5*m*Derivative(x(t), t)**2 + 0.5*(I + M*l**2)*Derivative(Θ(t), t)**2 + 0.5*(I + M*l**2)*Derivative(Φ(t), t)**2 + 0.5*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('----\\nThe Lagrangian of the system is:\\n----')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 3.5. The Euler-Lagrange Equations <a class=\"anchor\" id=\"euler-lagrange\"></a> [☝](#contents)\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial \\mathcal{L}}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force $Q^{P}$ acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial \\mathcal{L}}{\\partial x} = Q^{P}\n",
    "\\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial \\mathcal{L}}{\\partial x} = u\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = 0\n",
    "\\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial \\mathcal{L}}{\\partial \\phi} = 0\n",
    "\\tag{13}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broadband-arbitration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# euler-lagrange formulation\n",
    "\"\"\"\n",
    "`expand()`: allows cancellation of like terms\n",
    "`collect()`: collects common powers of a term in an expression\n",
    "\"\"\"\n",
    "\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), u)\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "The Euler-Lagrange equations:\n",
      "----\n",
      "1.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - 1.0 M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(1.0 M + 1.0 m + 1.0\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = u$"
      ],
      "text/plain": [
       "Eq(-2.0*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - 1.0*M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2.0*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (1.0*M + 1.0*m + 1.0)*Derivative(x(t), (t, 2)), u)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\nThe Euler-Lagrange equations:\\n----\\n1.\\n----')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "2.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 0.5 M l \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 0.5 M l \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2} + 1.0 l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 0.5*M*l*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 0.5*M*l*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2 + 1.0*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\n2.\\n----')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guided-attitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "3.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 0.5 M l \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 0.5 M l \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 0.5*M*l*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 0.5*M*l*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\n3.\\n----')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 3.6. Linearisation and Acceleration <a class=\"anchor\" id=\"linearisation\"></a> [☝](#contents)\n",
    "\n",
    "|  |  |  |\n",
    "| :-: | :-: | :-: |\n",
    "| $$\\sin(\\theta)$$ | $$\\approx$$ | $$\\theta$$ | \n",
    "|  $$\\cos(\\theta)$$ | $$=$$ | $$1$$ |\n",
    "| $$\\dot\\theta^{2}$$ | $$=$$ | $$0$$ |\n",
    "| $$\\sin(\\phi)$$ | $$\\approx$$ | $$\\phi$$ |\n",
    "| $$\\cos(\\phi)$$ | $$=$$ | $$1$$ |\n",
    "| $$\\dot\\phi^{2}$$ | $$=$$ | $$0$$ |\n",
    "| $$\\sin(\\theta - \\phi)$$ | $$\\approx$$ | $$\\theta - \\phi$$ |\n",
    "| $$\\cos(\\theta - \\phi)$$ | $$=$$ | $$1$$ |\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$. Using the above [small-angle approximations](https://brilliant.org/wiki/small-angle-approximation/) to simiplify the derived differential equations of motion, and solving for all three accelerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solid-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearise the system\n",
    "matrix = [(sin_theta, Θ), (cos_theta, 1), (Θ_dot**2, 0), \n",
    "         (sin_phi, Φ), (cos_phi, 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "The linearised equations are:\n",
      "----\n",
      "1.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2.0 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(1.0 M + 1.0 m + 1.0\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = u$"
      ],
      "text/plain": [
       "Eq(2.0*M*l*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(Φ(t), (t, 2)) + (1.0*M + 1.0*m + 1.0)*Derivative(x(t), (t, 2)), u)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\nThe linearised equations are:\\n----\\n1.\\n----')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "2.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + 0.5 M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2} + 1.0 l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 2.0*M*l*Derivative(x(t), (t, 2)) + 0.5*M*l*Derivative(Φ(t), (t, 2)) + (1.0*I + 1.0*M*l**2 + 1.0*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\n2.\\n----')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "3.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + 0.5 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l*Derivative(x(t), (t, 2)) + 0.5*M*l*Derivative(Θ(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\n3.\\n----')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, u, M, m, l, I)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, u, M, m, l, I)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, u, M, m, l, I)).simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "appropriate-tumor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Acceleration of the cart:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{- M^{2} g l^{2} \\left(4.0 I + 4.0 M l^{2} - 1.0 M l\\right) Θ{\\left(t \\right)} - 1.0 M^{2} g l^{2} \\left(I + M l^{2} - M l + l^{2}\\right) Φ{\\left(t \\right)} + u \\left(1.0 I^{2} + 2.0 I M l^{2} + 1.0 I l^{2} + 1.0 M^{2} l^{4} - 0.25 M^{2} l^{2} + 1.0 M l^{4}\\right)}{1.0 I^{2} + 1.0 I l^{2} + 1.0 I m \\left(I + l^{2}\\right) - M^{3} l^{2} \\left(4.0 l^{2} - 2.0 l + 0.25\\right) - M^{2} l^{2} \\left(3.0 I - 1.0 l^{2} m - 1.0 l^{2} + 0.25 m + 0.25\\right) + M \\left(1.0 I^{2} + 2.0 I l^{2} m + 3.0 I l^{2} + 1.0 l^{4} m + 1.0 l^{4}\\right)}$"
      ],
      "text/plain": [
       "(-M**2*g*l**2*(4.0*I + 4.0*M*l**2 - 1.0*M*l)*Θ(t) - 1.0*M**2*g*l**2*(I + M*l**2 - M*l + l**2)*Φ(t) + u*(1.0*I**2 + 2.0*I*M*l**2 + 1.0*I*l**2 + 1.0*M**2*l**4 - 0.25*M**2*l**2 + 1.0*M*l**4))/(1.0*I**2 + 1.0*I*l**2 + 1.0*I*m*(I + l**2) - M**3*l**2*(4.0*l**2 - 2.0*l + 0.25) - M**2*l**2*(3.0*I - 1.0*l**2*m - 1.0*l**2 + 0.25*m + 0.25) + M*(1.0*I**2 + 2.0*I*l**2*m + 3.0*I*l**2 + 1.0*l**4*m + 1.0*l**4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\nAcceleration of the cart:\\n----')\n",
    "x_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Acceleration of the first pendulum:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- M g l \\left(- 2.0 M l + 0.5 M + 0.5 m + 0.5\\right) Φ{\\left(t \\right)} + 2.0 g \\left(I M + I m + I + M l^{2} m + M l^{2}\\right) Θ{\\left(t \\right)} - u \\left(2.0 I + 2.0 M l^{2} - 0.5 M l\\right)\\right)}{1.0 I^{2} + 1.0 I l^{2} + 1.0 I m \\left(I + l^{2}\\right) - M^{3} l^{2} \\left(4.0 l^{2} - 2.0 l + 0.25\\right) - M^{2} l^{2} \\left(3.0 I - 1.0 l^{2} m - 1.0 l^{2} + 0.25 m + 0.25\\right) + M \\left(1.0 I^{2} + 2.0 I l^{2} m + 3.0 I l^{2} + 1.0 l^{4} m + 1.0 l^{4}\\right)}$"
      ],
      "text/plain": [
       "M*l*(-M*g*l*(-2.0*M*l + 0.5*M + 0.5*m + 0.5)*Φ(t) + 2.0*g*(I*M + I*m + I + M*l**2*m + M*l**2)*Θ(t) - u*(2.0*I + 2.0*M*l**2 - 0.5*M*l))/(1.0*I**2 + 1.0*I*l**2 + 1.0*I*m*(I + l**2) - M**3*l**2*(4.0*l**2 - 2.0*l + 0.25) - M**2*l**2*(3.0*I - 1.0*l**2*m - 1.0*l**2 + 0.25*m + 0.25) + M*(1.0*I**2 + 2.0*I*l**2*m + 3.0*I*l**2 + 1.0*l**4*m + 1.0*l**4))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\nAcceleration of the first pendulum:\\n----')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Acceleration of the second pendulum:\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- M g l \\left(- 4.0 M l + 1.0 M + 1.0 m + 1.0\\right) Θ{\\left(t \\right)} + g \\left(1.0 I M + 1.0 I m + 1.0 I - 3.0 M^{2} l^{2} + 1.0 M l^{2} m + 2.0 M l^{2} + 1.0 l^{2} m + 1.0 l^{2}\\right) Φ{\\left(t \\right)} - 1.0 u \\left(I + M l^{2} - M l + l^{2}\\right)\\right)}{1.0 I^{2} + 1.0 I l^{2} + 1.0 I m \\left(I + l^{2}\\right) - M^{3} l^{2} \\left(4.0 l^{2} - 2.0 l + 0.25\\right) - M^{2} l^{2} \\left(3.0 I - 1.0 l^{2} m - 1.0 l^{2} + 0.25 m + 0.25\\right) + M \\left(1.0 I^{2} + 2.0 I l^{2} m + 3.0 I l^{2} + 1.0 l^{4} m + 1.0 l^{4}\\right)}$"
      ],
      "text/plain": [
       "M*l*(-M*g*l*(-4.0*M*l + 1.0*M + 1.0*m + 1.0)*Θ(t) + g*(1.0*I*M + 1.0*I*m + 1.0*I - 3.0*M**2*l**2 + 1.0*M*l**2*m + 2.0*M*l**2 + 1.0*l**2*m + 1.0*l**2)*Φ(t) - 1.0*u*(I + M*l**2 - M*l + l**2))/(1.0*I**2 + 1.0*I*l**2 + 1.0*I*m*(I + l**2) - M**3*l**2*(4.0*l**2 - 2.0*l + 0.25) - M**2*l**2*(3.0*I - 1.0*l**2*m - 1.0*l**2 + 0.25*m + 0.25) + M*(1.0*I**2 + 2.0*I*l**2*m + 3.0*I*l**2 + 1.0*l**4*m + 1.0*l**4))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('----\\nAcceleration of the second pendulum:\\n----')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 4. Proximal Policy Optimisation <a class=\"anchor\" id=\"ppo\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-chess",
   "metadata": {},
   "source": [
    "### 4.1. Overview<sup>1</sup> <a class=\"anchor\" id=\"ppo-overview\"></a> [☝](#contents)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    "\n",
    "---\n",
    "<sup>1</sup>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-elizabeth",
   "metadata": {},
   "source": [
    "### 4.2. Mathematical Model<sup>1</sup>  <a class=\"anchor\" id=\"ppo-math\"></a> [☝](#contents)\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ <br />\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp; - $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.<br />\n",
    "&emsp;&emsp;&emsp;&emsp; - $\\epsilon$: Hyper-parameter; usually with a value of 0.2.<br />\n",
    "&emsp;&emsp;&emsp;&emsp; - clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.<br />\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution.\n",
    "\n",
    "---\n",
    "<sup>1</sup><span id=\"mm\"></span>Referenced from [Rich Sutton et al.](http://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 4.3. Neural Network (NN) — A2C<sup>1</sup><a class=\"anchor\" id=\"ppo-nn\"></a> [☝](#contents)\n",
    "\n",
    "<img src=\"images/ac.jpg\" width=\"500\" />\n",
    "<h5 align=\"center\"> Figure: Advantage Actor Critic (A2C) Architecture<sup>2</sup></h5> \n",
    "\n",
    "The input is a representation of the current state. One output is a probability distribution over moves — **the actor**. The other output represents the expected return from the current position — **the critic**.<sup>3</sup>\n",
    "\n",
    "* Each output serves as a sort of regularizer on the other[regularization is any technique to prevent your model from overfitting to the exact data set it was trained on] \n",
    "* Taking the game of Go as an example, imagine that a group of stones on the board is in danger of getting captured. This fact is relevant for the value output, because the player with the weak stones is probably behind. It’s also relevant to the action output, because you probably want to either attack or defend the weak stones. If your network learns a “weak stone” detector in the early layers, that’s relevant to both outputs. Training on both outputs forces the network to learn a representation that’s useful for both goals. This can often improve generalization and sometimes even speed up training.\n",
    "\n",
    "\n",
    "Actor-critic methods exhibit two significant advantages<sup>2</sup>:\n",
    "\n",
    "- They require minimal computation in order to select actions. Consider a case where there are an infinite number of possible actions — for example, a continuous-valued action. Any method learning just action values must search through this infinite set in order to pick an action. If the policy is explicitly stored, then this extensive computation may not be needed for each action selection.\n",
    "- They can learn an explicitly stochastic policy; that is, they can learn the optimal probabilities of selecting various actions. This ability turns out to be useful in competitive and non-Markov cases (e.g., see Singh, Jaakkola, and Jordan, 1994).\n",
    "\n",
    "-----\n",
    "<sup>1</sup>Code referenced from [Machine Learning With Phil](https://github.com/philtabor/Youtube-Code-Repository)<br /> \n",
    "<sup>2</sup>Referenced from [Deep Learning and the Game of Go](https://livebook.manning.com/book/deep-learning-and-the-game-of-go/chapter-12/)<br /> \n",
    "<sup>3</sup>Referenced from [Actor-Critic Methods](http://incompleteideas.net/book/first/ebook/node66.html)<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-malta",
   "metadata": {},
   "source": [
    "#### 4.3.1. Actor <a class=\"anchor\" id=\"ppo-nn-actor\"></a> [☝](#contents)\n",
    "\n",
    "The “Actor” updates the policy distribution in the direction suggested by the Critic (such as with policy gradients).<sup>1</sup>\n",
    " \n",
    "---\n",
    "<sup>1</sup>Referenced from [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "julian-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        # neural network setup\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the NN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-christopher",
   "metadata": {},
   "source": [
    "#### 4.3.2. Critic <a class=\"anchor\" id=\"ppo-nn-critic\"></a> [☝](#contents)\n",
    "\n",
    "The “Critic” estimates the value function. This could be the action-value (the Q value) or state-value (the V value).<sup>1</sup>\n",
    "\n",
    "---\n",
    "<sup>1</sup>Referenced from [Understanding Actor Critic Methods and A2C](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "armed-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NOTE: See the above comments in the `ActorNetwork` for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-salon",
   "metadata": {},
   "source": [
    "#### 4.3.3. Agent <sup>1</sup><a class=\"anchor\" id=\"ppo-nn-agent\"></a> [☝](#contents)\n",
    " \n",
    "The agent is the component that makes the *decision* of what action to take.\n",
    "\n",
    "* The agent is allowed to use any observation from the environment, and any internal rules that it has. Those internal rules can be anything, but typically, it expects the current state to be provided by the environment, for that state to utilise the [Markov chain](https://en.wikipedia.org/wiki/Markov_chain), and then it processes that state using a policy function that decides what action to take.\n",
    "\n",
    "* In addition, to handle a reward signal (received from the environment) and optimise the agent towards maximising the expected reward in future, the agent will maintain some data which is influenced by the rewards it received in the past, and use that to construct a better policy.\n",
    "\n",
    "---\n",
    "<sup>1</sup>Referenced from [Neil Slater](https://ai.stackexchange.com/questions/8476/what-does-the-agent-in-reinforcement-learning-exactly-do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage class\n",
    "class ExperienceCollector:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = ExperienceCollector(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> the advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 4.4. Environment <a class=\"anchor\" id=\"ppo-env\"></a> [☝](#contents)\n",
    "\n",
    "The environment is a modelled as a stochastic finite state machine with inputs (actions sent from the agent) and outputs (observations and rewards sent to the agent).<sup>1</sup>\n",
    "\n",
    "The agent-environment boundary can be located at different places for different purposes. In practice, the agent-environment boundary is determined once one has selected particular states, actions, and rewards, and thus has identified a specific decision-making task of interest.<sup>2</sup>\n",
    "\n",
    "---\n",
    "<sup>1</sup>Referenced from [A Brief Introduction to Reinforcement Learning](https://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html)<br />\n",
    "<sup>2</sup>Referenced from [Reinforcement Learning:\n",
    "An Introduction](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "\n",
    "---\n",
    "The simplified acceleration of each component from [Section 3.6: Linerisation and Acceleration](#linearisation):\n",
    "\n",
    "\\begin{equation}\n",
    "\\ddot{x}=\\frac{\\begin{array}{l}-M^2{gl}^2\\left[\\theta{}\\left(4I+Ml\\left(4l-1\\right)\\right)+\\phi{}\\left(I+Ml\\left(l-1\\right)+l^2\\right)\\right]\\\\+u\\left(I^2+Il^2\\left(2M+1\\right)+Ml^2\\left(Ml^2-\\frac{1}{4}M+l^2\\right)\\right)\\end{array}}{X}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\ddot{\\theta{}}=\\frac{Ml\\left[\\begin{array}{l}-2Mgl\\left(\\frac{1}{4}\\left(M+m+1\\right)-Ml\\right)\\phi{}\\\\+2g\\left(I\\left(M+m+1\\right)+Ml^2(m+1\\right)\\theta{}\\\\-2u\\left(I+Ml\\left(l-\\frac{1}{4}\\right)\\right)\\end{array}\\right]}{X}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\ddot{\\phi{}}=\\frac{Ml\\left[\\begin{array}{l}-Mgl\\left(-M\\left(4l-1\\right)+m+1\\right)\\theta{}\\\\+g\\left(I\\left(M+m+1\\right)+Ml^2\\left(m-3M+\\left(\\frac{m+1}{M}\\right)+2\\right)\\right)\\phi{}\\\\-u\\left(I(I+1)+Ml\\left(l-1\\right)\\right)\\end{array}\\right]}{X}\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\\begin{equation}\n",
    "X=I\\left[I+l^2+m\\left(I+l^2\\right)\\right]-M^2l^2\\left[M\\left(2l\\left(2l-1\\right)+\\frac{1}{4}\\right)\\\\+\\left(3I-l^2\\left(m-1\\right)+\\frac{1}{4}\\left(m+1\\right)\\right)\\right]+M\\left(I^2+l^2\\left[I\\left(2m+3\\right)+l^2(m+1)\\right]\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the classic cart-pole system implemented by Rich Sutton et al.\n",
    "Original `CartPole-v0` environment available here: http://incompleteideas.net/sutton/book/code/pole.c\n",
    "Permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "class DoubleInvertedPendulum(gym.Env): \n",
    "    def __init__(self):                                             \n",
    "        self.gravity = 9.81                                          # ms^2\n",
    "        self.mass_cart = 1.0                                         # kg\n",
    "        self.masspole_1 = 0.1                                        # kg\n",
    "        self.masspole_2 = 0.1                                        # kg\n",
    "        self.mass_pole = (self.masspole_1 + self.masspole_2)         # kg\n",
    "        self.lengthpole_1 = 0.25                                     # m\n",
    "        self.lengthpole_2 = 0.25                                     # m\n",
    "        self.length = (self.lengthpole_1 + self.lengthpole_2)        # m\n",
    "        self.force = 10.0                                            # N\n",
    "        self.moment_inertia = (self.mass_pole*self.length**2) / 12   # kgm^2\n",
    "        self.tau = 0.02                                              # s\n",
    "        self.kinematics_integrator = 'euler'\n",
    "        self.theta = 0.05\n",
    "        self.phi = 0.05\n",
    "\n",
    "        # angle at which to fail the episode - ~ 0.2 rad, ~12 deg \n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.phi_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        # distance of cart to fail episode\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.phi_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot, phi, phi_dot = self.state\n",
    "        force = self.force if action == 1 else -self.force\n",
    "        \n",
    "        #################################\n",
    "        \n",
    "        # denominator\n",
    "        denominator_X = (self.moment_inertia**2) + (self.moment_inertia*self.length**2) + \\\n",
    "        (self.moment_inertia*self.mass_cart*(self.moment_inertia+self.length**2)) - \\\n",
    "        (self.mass_pole**3*self.length**2*(4*self.length**2-2*self.length+0.25)) - \\\n",
    "        (self.mass_pole**2*self.length**2*(3*self.moment_inertia-self.length**2*self.mass_cart-self.length**2\\\n",
    "                                           +0.25*self.mass_cart+0.25)) + \\\n",
    "        (self.mass_pole*(self.moment_inertia**2+2*self.moment_inertia*self.length**2*self.mass_cart\\\n",
    "                         +3*self.moment_inertia*self.length**2+self.length**4*self.mass_cart+self.length**4))\n",
    "        \n",
    "        # x-acceleration numerator\n",
    "        numerator_x_acc = (self.force*(self.moment_inertia**2+2*self.moment_inertia*self.mass_pole*self.length**2\\\n",
    "                                       +self.moment_inertia*self.length**2+ \\\n",
    "                                 self.mass_pole**2*self.length**4-0.25*self.mass_pole**2*self.length**2\\\n",
    "                                       +self.mass_pole*self.length**4)) - \\\n",
    "        (self.mass_pole**2*self.gravity*self.length**2*(4*self.moment_inertia+4*self.mass_pole**2\\\n",
    "                                                        -self.mass_pole*self.length)*self.theta) - \\\n",
    "        (self.mass_pole**2*self.gravity*self.length**2*(self.moment_inertia+self.mass_pole*self.length**2\\\n",
    "                                                        -self.mass_pole*self.length+self.length**2)*self.phi)\n",
    "        \n",
    "        x_acc = numerator_x_acc / denominator_X\n",
    "        \n",
    "        # theta-acceleration numerator\n",
    "        numerator_theta_acc = (self.mass_pole*self.length*(-self.mass_pole*self.gravity*self.length*\\\n",
    "                                                           (-2*self.mass_pole*self.length+0.5*self.mass_pole\\\n",
    "                                                            +0.5*self.mass_cart+0.2)*self.phi) + \\\n",
    "                              2*self.gravity*(self.moment_inertia*self.mass_pole+self.moment_inertia*self.mass_cart\\\n",
    "                                              +self.moment_inertia+\\\n",
    "                                        self.mass_pole*self.mass_cart*self.length**2+self.mass_pole*self.length**2)\\\n",
    "                               *self.theta - \\\n",
    "                              self.force*(2*self.moment_inertia+2*self.mass_pole*self.length**2-\\\n",
    "                                          0.5*self.mass_pole*self.length))\n",
    "        \n",
    "        theta_acc = numerator_theta_acc / denominator_X\n",
    "        \n",
    "        # phi-acceleration numerator\n",
    "        numerator_phi_acc = (self.mass_pole*self.length*(-self.mass_pole*self.gravity*self.length*\\\n",
    "                                                         (-4*self.mass_pole*self.length+self.mass_pole+self.mass_cart+1)\\\n",
    "                                                         *self.theta) + \\\n",
    "                            (self.gravity*(self.moment_inertia*self.mass_pole+self.moment_inertia*self.mass_cart\\\n",
    "                                           +self.moment_inertia-3*self.mass_pole**2*self.length**2+\\\n",
    "                               self.mass_pole*self.length**2*self.mass_cart+2*self.mass_pole*self.length**2\\\n",
    "                                           +self.length**2*self.mass_cart+self.length**2)*self.phi - \\\n",
    "                            self.force*(self.moment_inertia+self.mass_pole*self.length**2-self.mass_pole*self.length\\\n",
    "                                        +self.moment_inertia**2)))\n",
    "\n",
    "        phi_acc = numerator_phi_acc / denominator_X\n",
    "        \n",
    "        #################################\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * x_acc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi = phi + self.tau * phi_dot\n",
    "            theta_dot = theta_dot + self.tau * theta_acc\n",
    "            phi_dot = phi_dot + self.tau * phi_acc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * x_acc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * theta_acc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi_dot = phi_dot + self.tau * phi_acc\n",
    "            phi = phi + self.tau * phi_dot  \n",
    "            \n",
    "        self.state = (x, x_dot, theta, theta_dot, phi, phi_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "            or phi < -self.phi_threshold_radians\n",
    "            or phi > self.phi_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(6,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 4.5. Test <a class=\"anchor\" id=\"ppo-test\"></a> [☝](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lesbian-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to measure time taken by each test run\n",
    "def exec_time(start, end):\n",
    "   diff_time = end - start\n",
    "   m, s = divmod(diff_time, 60)\n",
    "   h, m = divmod(m, 60)\n",
    "   s,m,h = int(round(s, 0)), int(round(m, 0)), int(round(h, 0))\n",
    "   print(\"Execution Time: \" + \"{0:02d}:{1:02d}:{2:02d}\".format(h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start time of each test run\n",
    "start = time.time()\n",
    "\n",
    "# create environment\n",
    "env = DoubleInvertedPendulum()\n",
    "# number of samples processed before the model is updated\n",
    "batch_size = 5 \n",
    "# a full training pass over the entire dataset such that each example has been seen once\n",
    "num_epochs = 4 \n",
    "# controls the rate or speed at which the model learns\n",
    "learning_rate_alpha = 3e-4\n",
    "\n",
    "# create agent\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "# record score history\n",
    "score_history = []\n",
    "# a reward function that informs the agent how well its current actions and states are doing\n",
    "learn_iters = 0\n",
    "# track average score\n",
    "average_score = 0\n",
    "# number of steps means using one batch size of training data to train the model\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        # get information back from environment\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        # update step\n",
    "        num_steps += 1\n",
    "        # update score based on current reward\n",
    "        score += reward\n",
    "\n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % num_steps == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "    \n",
    "    # format output\n",
    "    if i+1 < 10:\n",
    "        print('episode: ', i+1, '  |  score: %.0f' % score)\n",
    "    elif i+1 < 100:\n",
    "        print('episode: ', i+1, ' |  score: %.0f' % score)   \n",
    "    else:\n",
    "        print('episode: ', i+1, '|  score: %.0f' % score)  \n",
    "    \n",
    "episodes = [i+1 for i in range(len(score_history))]\n",
    "# end time of each test run\n",
    "end = time.time()\n",
    "# display time taken by test run\n",
    "print('---------')\n",
    "exec_time(start,end)\n",
    "print('---------')\n",
    "print(f\"Total reward for {num_games} unsupervised episodes: {sum(score_history)}\")\n",
    "print('---------')\n",
    "\n",
    "# visualise learning\n",
    "def plot_learning_curve(episode, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(episode, running_avg)\n",
    "    plt.title(f\"Unsupervised learning for {num_games} games\", fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward / episode', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(episodes, score_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-jamaica",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a class=\"anchor\" id=\"conclusion\"></a> [☝](#contents)\n",
    "\n",
    "Various starting angle combinations were tried. Additionally, different parameters were modulated to observe the agent's behaviour.\n",
    "\n",
    "#### Initial Parameters:\n",
    "\n",
    "<img src=\"images/tests/init_params.png\" align=\"center\" alt=\"initialisation_parameters\" style=\"width: 500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-browser",
   "metadata": {},
   "source": [
    "### 5.1. Variations in initial angle conditions <a class=\"anchor\" id=\"conclusion-init-angles\"></a> [☝](#contents)\n",
    "\n",
    "* Total of **10,000** games were played.\n",
    "\n",
    "| $$\\theta$$ | $$\\phi$$ | Total reward | Total time $$(min)$$ | Accuracy $$(\\%)$$ |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| 0.05 | 0.05 | 72,283 | 29.14 | **72.30** |\n",
    "| 0.10 | 0.10 | 62,919 | 27.45 | **62.92** |\n",
    "| 0.15 | 0.15 | 56,180 | 17.28 | **56.18** |\n",
    "| 0.05 | 0.15 | 62,744 | 22.12 | **62.74** |\n",
    "| 0.15 | 0.05 | 72,783 | 23.40 | **72.78** |\n",
    "| 0.00 | 0.15 | 55,859 | 17.44 | **55.86** |\n",
    "| 0.15 | 0.00 | 91,709 | 28.11 | **91.71** |\n",
    "\n",
    "\n",
    "|  |  |\n",
    "| :-: | :-: | \n",
    "| <img src=\"images/reward_aggregation.png\" alt=\"reward_aggregation\" style=\"width: 500px\"/> | <img src=\"images/time_aggregation.png\" alt=\"time_aggregation\" style=\"width: 500px\"/> | \n",
    "|  |  |\n",
    "| <img src=\"images/total_rewards.png\" alt=\"total_rewards\" style=\"width: 500px\"/> | <img src=\"images/total_time.png\" alt=\"total_time\" style=\"width: 500px\"/> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
