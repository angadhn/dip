{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket initialisation stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, done):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, done)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Introduce second pendulum\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 10.0 |\n",
      "| episode:  1  | score: 16.0 |\n",
      "| episode:  2  | score: 11.0 |\n",
      "| episode:  3  | score: 15.0 |\n",
      "| episode:  4  | score: 9.0 |\n",
      "| episode:  5  | score: 10.0 |\n",
      "| episode:  6  | score: 9.0 |\n",
      "| episode:  7  | score: 8.0 |\n",
      "| episode:  8  | score: 9.0 |\n",
      "| episode:  9  | score: 9.0 |\n",
      "| episode:  10  | score: 12.0 |\n",
      "| episode:  11  | score: 9.0 |\n",
      "| episode:  12  | score: 9.0 |\n",
      "| episode:  13  | score: 12.0 |\n",
      "| episode:  14  | score: 9.0 |\n",
      "| episode:  15  | score: 9.0 |\n",
      "| episode:  16  | score: 10.0 |\n",
      "| episode:  17  | score: 8.0 |\n",
      "| episode:  18  | score: 10.0 |\n",
      "| episode:  19  | score: 9.0 |\n",
      "| episode:  20  | score: 9.0 |\n",
      "| episode:  21  | score: 9.0 |\n",
      "| episode:  22  | score: 9.0 |\n",
      "| episode:  23  | score: 9.0 |\n",
      "| episode:  24  | score: 10.0 |\n",
      "| episode:  25  | score: 9.0 |\n",
      "| episode:  26  | score: 8.0 |\n",
      "| episode:  27  | score: 10.0 |\n",
      "| episode:  28  | score: 9.0 |\n",
      "| episode:  29  | score: 8.0 |\n",
      "| episode:  30  | score: 8.0 |\n",
      "| episode:  31  | score: 11.0 |\n",
      "| episode:  32  | score: 10.0 |\n",
      "| episode:  33  | score: 9.0 |\n",
      "| episode:  34  | score: 8.0 |\n",
      "| episode:  35  | score: 9.0 |\n",
      "| episode:  36  | score: 9.0 |\n",
      "| episode:  37  | score: 10.0 |\n",
      "| episode:  38  | score: 9.0 |\n",
      "| episode:  39  | score: 10.0 |\n",
      "| episode:  40  | score: 10.0 |\n",
      "| episode:  41  | score: 10.0 |\n",
      "| episode:  42  | score: 8.0 |\n",
      "| episode:  43  | score: 9.0 |\n",
      "| episode:  44  | score: 10.0 |\n",
      "| episode:  45  | score: 9.0 |\n",
      "| episode:  46  | score: 12.0 |\n",
      "| episode:  47  | score: 8.0 |\n",
      "| episode:  48  | score: 10.0 |\n",
      "| episode:  49  | score: 10.0 |\n",
      "| episode:  50  | score: 8.0 |\n",
      "| episode:  51  | score: 9.0 |\n",
      "| episode:  52  | score: 8.0 |\n",
      "| episode:  53  | score: 10.0 |\n",
      "| episode:  54  | score: 9.0 |\n",
      "| episode:  55  | score: 9.0 |\n",
      "| episode:  56  | score: 9.0 |\n",
      "| episode:  57  | score: 10.0 |\n",
      "| episode:  58  | score: 12.0 |\n",
      "| episode:  59  | score: 13.0 |\n",
      "| episode:  60  | score: 11.0 |\n",
      "| episode:  61  | score: 8.0 |\n",
      "| episode:  62  | score: 13.0 |\n",
      "| episode:  63  | score: 9.0 |\n",
      "| episode:  64  | score: 14.0 |\n",
      "| episode:  65  | score: 13.0 |\n",
      "| episode:  66  | score: 12.0 |\n",
      "| episode:  67  | score: 13.0 |\n",
      "| episode:  68  | score: 10.0 |\n",
      "| episode:  69  | score: 9.0 |\n",
      "| episode:  70  | score: 9.0 |\n",
      "| episode:  71  | score: 11.0 |\n",
      "| episode:  72  | score: 11.0 |\n",
      "| episode:  73  | score: 12.0 |\n",
      "| episode:  74  | score: 11.0 |\n",
      "| episode:  75  | score: 16.0 |\n",
      "| episode:  76  | score: 10.0 |\n",
      "| episode:  77  | score: 8.0 |\n",
      "| episode:  78  | score: 19.0 |\n",
      "| episode:  79  | score: 27.0 |\n",
      "| episode:  80  | score: 10.0 |\n",
      "| episode:  81  | score: 16.0 |\n",
      "| episode:  82  | score: 21.0 |\n",
      "| episode:  83  | score: 29.0 |\n",
      "| episode:  84  | score: 36.0 |\n",
      "| episode:  85  | score: 22.0 |\n",
      "| episode:  86  | score: 12.0 |\n",
      "| episode:  87  | score: 9.0 |\n",
      "| episode:  88  | score: 34.0 |\n",
      "| episode:  89  | score: 16.0 |\n",
      "| episode:  90  | score: 52.0 |\n",
      "| episode:  91  | score: 36.0 |\n",
      "| episode:  92  | score: 28.0 |\n",
      "| episode:  93  | score: 31.0 |\n",
      "| episode:  94  | score: 40.0 |\n",
      "| episode:  95  | score: 33.0 |\n",
      "| episode:  96  | score: 57.0 |\n",
      "| episode:  97  | score: 28.0 |\n",
      "| episode:  98  | score: 83.0 |\n",
      "| episode:  99  | score: 50.0 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = CartPoleEnv()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.1f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('Average reward', fontsize=11)\n",
    "    plt.ylabel('No. of games', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
