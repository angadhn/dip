{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "important-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from gym import error\n",
    "\n",
    "def np_random(seed=None):\n",
    "    if seed is not None and not (isinstance(seed, int) and 0 <= seed):\n",
    "        raise error.Error('Seed must be a non-negative integer or omitted, not {}'.format(seed))\n",
    "\n",
    "    seed = create_seed(seed)\n",
    "\n",
    "    rng = np.random.RandomState()\n",
    "    rng.seed(_int_list_from_bigint(hash_seed(seed)))\n",
    "    return rng, seed\n",
    "\n",
    "def hash_seed(seed=None, max_bytes=8):\n",
    "    \"\"\"Any given evaluation is likely to have many PRNG's active at\n",
    "    once. (Most commonly, because the environment is running in\n",
    "    multiple processes.) There's literature indicating that having\n",
    "    linear correlations between seeds of multiple PRNG's can correlate\n",
    "    the outputs:\n",
    "    http://blogs.unity3d.com/2015/01/07/a-primer-on-repeatable-random-numbers/\n",
    "    http://stackoverflow.com/questions/1554958/how-different-do-random-seeds-need-to-be\n",
    "    http://dl.acm.org/citation.cfm?id=1276928\n",
    "    Thus, for sanity we hash the seeds before using them. (This scheme\n",
    "    is likely not crypto-strength, but it should be good enough to get\n",
    "    rid of simple correlations.)\n",
    "    Args:\n",
    "        seed (Optional[int]): None seeds from an operating system specific randomness source.\n",
    "        max_bytes: Maximum number of bytes to use in the hashed seed.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = create_seed(max_bytes=max_bytes)\n",
    "    hash = hashlib.sha512(str(seed).encode('utf8')).digest()\n",
    "    return _bigint_from_bytes(hash[:max_bytes])\n",
    "\n",
    "def create_seed(a=None, max_bytes=8):\n",
    "    \"\"\"Create a strong random seed. Otherwise, Python 2 would seed using\n",
    "    the system time, which might be non-robust especially in the\n",
    "    presence of concurrency.\n",
    "    Args:\n",
    "        a (Optional[int, str]): None seeds from an operating system specific randomness source.\n",
    "        max_bytes: Maximum number of bytes to use in the seed.\n",
    "    \"\"\"\n",
    "    # Adapted from https://svn.python.org/projects/python/tags/r32/Lib/random.py\n",
    "    if a is None:\n",
    "        a = _bigint_from_bytes(os.urandom(max_bytes))\n",
    "    elif isinstance(a, str):\n",
    "        a = a.encode('utf8')\n",
    "        a += hashlib.sha512(a).digest()\n",
    "        a = _bigint_from_bytes(a[:max_bytes])\n",
    "    elif isinstance(a, int):\n",
    "        a = a % 2**(8 * max_bytes)\n",
    "    else:\n",
    "        raise error.Error('Invalid type for seed: {} ({})'.format(type(a), a))\n",
    "\n",
    "    return a\n",
    "\n",
    "# TODO: don't hardcode sizeof_int here\n",
    "def _bigint_from_bytes(bytes):\n",
    "    sizeof_int = 4\n",
    "    padding = sizeof_int - len(bytes) % sizeof_int\n",
    "    bytes += b'\\0' * padding\n",
    "    int_count = int(len(bytes) / sizeof_int)\n",
    "    unpacked = struct.unpack(\"{}I\".format(int_count), bytes)\n",
    "    accum = 0\n",
    "    for i, val in enumerate(unpacked):\n",
    "        accum += 2 ** (sizeof_int * 8 * i) * val\n",
    "    return accum\n",
    "\n",
    "def _int_list_from_bigint(bigint):\n",
    "    # Special case 0\n",
    "    if bigint < 0:\n",
    "        raise error.Error('Seed must be non-negative, not {}'.format(bigint))\n",
    "    elif bigint == 0:\n",
    "        return [0]\n",
    "\n",
    "    ints = []\n",
    "    while bigint > 0:\n",
    "        bigint, mod = divmod(bigint, 2 ** 32)\n",
    "        ints.append(mod)\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole_1 = 0.1\n",
    "        self.masspole_2 = 0.1\n",
    "        self.masspole = (self.masspole_1 + self.masspole_2)\n",
    "        self.total_mass = (self.masscart + self.masspole)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.phi_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        # distance of cart to fail episode\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.phi_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot, phi, phi_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot, phi, phi_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(6,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 23.00 |\n",
      "| episode:  1  | score: 17.00 |\n",
      "| episode:  2  | score: 50.00 |\n",
      "| episode:  3  | score: 9.00 |\n",
      "| episode:  4  | score: 15.00 |\n",
      "| episode:  5  | score: 31.00 |\n",
      "| episode:  6  | score: 22.00 |\n",
      "| episode:  7  | score: 24.00 |\n",
      "| episode:  8  | score: 17.00 |\n",
      "| episode:  9  | score: 11.00 |\n",
      "| episode:  10  | score: 27.00 |\n",
      "| episode:  11  | score: 22.00 |\n",
      "| episode:  12  | score: 12.00 |\n",
      "| episode:  13  | score: 16.00 |\n",
      "| episode:  14  | score: 18.00 |\n",
      "| episode:  15  | score: 20.00 |\n",
      "| episode:  16  | score: 14.00 |\n",
      "| episode:  17  | score: 27.00 |\n",
      "| episode:  18  | score: 14.00 |\n",
      "| episode:  19  | score: 15.00 |\n",
      "| episode:  20  | score: 19.00 |\n",
      "| episode:  21  | score: 30.00 |\n",
      "| episode:  22  | score: 56.00 |\n",
      "| episode:  23  | score: 59.00 |\n",
      "| episode:  24  | score: 38.00 |\n",
      "| episode:  25  | score: 15.00 |\n",
      "| episode:  26  | score: 18.00 |\n",
      "| episode:  27  | score: 34.00 |\n",
      "| episode:  28  | score: 13.00 |\n",
      "| episode:  29  | score: 21.00 |\n",
      "| episode:  30  | score: 27.00 |\n",
      "| episode:  31  | score: 18.00 |\n",
      "| episode:  32  | score: 19.00 |\n",
      "| episode:  33  | score: 15.00 |\n",
      "| episode:  34  | score: 17.00 |\n",
      "| episode:  35  | score: 26.00 |\n",
      "| episode:  36  | score: 17.00 |\n",
      "| episode:  37  | score: 12.00 |\n",
      "| episode:  38  | score: 33.00 |\n",
      "| episode:  39  | score: 49.00 |\n",
      "| episode:  40  | score: 35.00 |\n",
      "| episode:  41  | score: 21.00 |\n",
      "| episode:  42  | score: 22.00 |\n",
      "| episode:  43  | score: 49.00 |\n",
      "| episode:  44  | score: 24.00 |\n",
      "| episode:  45  | score: 49.00 |\n",
      "| episode:  46  | score: 47.00 |\n",
      "| episode:  47  | score: 43.00 |\n",
      "| episode:  48  | score: 26.00 |\n",
      "| episode:  49  | score: 40.00 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = CartPoleEnv()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 50\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # render(env)\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.2f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-nudist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
