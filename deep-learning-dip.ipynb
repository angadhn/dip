{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket initialisation stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, done):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, done)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Introduce second pendulum\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 12.0 |\n",
      "| episode:  1  | score: 33.0 |\n",
      "| episode:  2  | score: 11.0 |\n",
      "| episode:  3  | score: 11.0 |\n",
      "| episode:  4  | score: 9.0 |\n",
      "| episode:  5  | score: 11.0 |\n",
      "| episode:  6  | score: 11.0 |\n",
      "| episode:  7  | score: 13.0 |\n",
      "| episode:  8  | score: 12.0 |\n",
      "| episode:  9  | score: 10.0 |\n",
      "| episode:  10  | score: 11.0 |\n",
      "| episode:  11  | score: 13.0 |\n",
      "| episode:  12  | score: 17.0 |\n",
      "| episode:  13  | score: 8.0 |\n",
      "| episode:  14  | score: 9.0 |\n",
      "| episode:  15  | score: 10.0 |\n",
      "| episode:  16  | score: 11.0 |\n",
      "| episode:  17  | score: 11.0 |\n",
      "| episode:  18  | score: 9.0 |\n",
      "| episode:  19  | score: 9.0 |\n",
      "| episode:  20  | score: 10.0 |\n",
      "| episode:  21  | score: 11.0 |\n",
      "| episode:  22  | score: 10.0 |\n",
      "| episode:  23  | score: 10.0 |\n",
      "| episode:  24  | score: 8.0 |\n",
      "| episode:  25  | score: 10.0 |\n",
      "| episode:  26  | score: 9.0 |\n",
      "| episode:  27  | score: 9.0 |\n",
      "| episode:  28  | score: 10.0 |\n",
      "| episode:  29  | score: 10.0 |\n",
      "| episode:  30  | score: 10.0 |\n",
      "| episode:  31  | score: 14.0 |\n",
      "| episode:  32  | score: 12.0 |\n",
      "| episode:  33  | score: 15.0 |\n",
      "| episode:  34  | score: 9.0 |\n",
      "| episode:  35  | score: 14.0 |\n",
      "| episode:  36  | score: 12.0 |\n",
      "| episode:  37  | score: 23.0 |\n",
      "| episode:  38  | score: 17.0 |\n",
      "| episode:  39  | score: 12.0 |\n",
      "| episode:  40  | score: 16.0 |\n",
      "| episode:  41  | score: 15.0 |\n",
      "| episode:  42  | score: 19.0 |\n",
      "| episode:  43  | score: 19.0 |\n",
      "| episode:  44  | score: 22.0 |\n",
      "| episode:  45  | score: 22.0 |\n",
      "| episode:  46  | score: 30.0 |\n",
      "| episode:  47  | score: 35.0 |\n",
      "| episode:  48  | score: 39.0 |\n",
      "| episode:  49  | score: 39.0 |\n",
      "| episode:  50  | score: 69.0 |\n",
      "| episode:  51  | score: 84.0 |\n",
      "| episode:  52  | score: 63.0 |\n",
      "| episode:  53  | score: 26.0 |\n",
      "| episode:  54  | score: 38.0 |\n",
      "| episode:  55  | score: 11.0 |\n",
      "| episode:  56  | score: 25.0 |\n",
      "| episode:  57  | score: 119.0 |\n",
      "| episode:  58  | score: 84.0 |\n",
      "| episode:  59  | score: 71.0 |\n",
      "| episode:  60  | score: 38.0 |\n",
      "| episode:  61  | score: 27.0 |\n",
      "| episode:  62  | score: 23.0 |\n",
      "| episode:  63  | score: 29.0 |\n",
      "| episode:  64  | score: 24.0 |\n",
      "| episode:  65  | score: 22.0 |\n",
      "| episode:  66  | score: 68.0 |\n",
      "| episode:  67  | score: 110.0 |\n",
      "| episode:  68  | score: 251.0 |\n",
      "| episode:  69  | score: 70.0 |\n",
      "| episode:  70  | score: 103.0 |\n",
      "| episode:  71  | score: 102.0 |\n",
      "| episode:  72  | score: 277.0 |\n",
      "| episode:  73  | score: 106.0 |\n",
      "| episode:  74  | score: 134.0 |\n",
      "| episode:  75  | score: 61.0 |\n",
      "| episode:  76  | score: 106.0 |\n",
      "| episode:  77  | score: 32.0 |\n",
      "| episode:  78  | score: 54.0 |\n",
      "| episode:  79  | score: 58.0 |\n",
      "| episode:  80  | score: 40.0 |\n",
      "| episode:  81  | score: 34.0 |\n",
      "| episode:  82  | score: 105.0 |\n",
      "| episode:  83  | score: 69.0 |\n",
      "| episode:  84  | score: 41.0 |\n",
      "| episode:  85  | score: 35.0 |\n",
      "| episode:  86  | score: 30.0 |\n",
      "| episode:  87  | score: 24.0 |\n",
      "| episode:  88  | score: 26.0 |\n",
      "| episode:  89  | score: 28.0 |\n",
      "| episode:  90  | score: 19.0 |\n",
      "| episode:  91  | score: 24.0 |\n",
      "| episode:  92  | score: 29.0 |\n",
      "| episode:  93  | score: 39.0 |\n",
      "| episode:  94  | score: 40.0 |\n",
      "| episode:  95  | score: 33.0 |\n",
      "| episode:  96  | score: 35.0 |\n",
      "| episode:  97  | score: 23.0 |\n",
      "| episode:  98  | score: 27.0 |\n",
      "| episode:  99  | score: 20.0 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = CartPoleEnv()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.1f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
