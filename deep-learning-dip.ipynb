{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the classic cart-pole system implemented by Rich Sutton et al.\n",
    "\"\"\"\n",
    "\n",
    "class DoubleInvertedPendulumMA(gym.Env): \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole_1 = 0.1\n",
    "        self.masspole_2 = 0.1\n",
    "        self.masspole = (self.masspole_1 + self.masspole_2)\n",
    "        self.total_mass = (self.masscart + self.masspole)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.phi_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        # distance of cart to fail episode\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.phi_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot, phi, phi_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        cosphi = math.cos(phi)\n",
    "        sinphi = math.cos(phi)\n",
    "        \n",
    "        # TODO: double-check the equations\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        \n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "\n",
    "        phiacc = (self.gravity * sinphi - sinphi * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * cosphi ** 2 / self.total_mass))\n",
    "        \n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi = phi + self.tau * phi_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            # print(f\"phi_dot: {phi_dot}, theta_dot{theta_dot}\")\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            phi = phi + self.tau * phi_dot  \n",
    "            \n",
    "        self.state = (x, x_dot, theta, theta_dot, phi, phi_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "            or phi < -self.phi_threshold_radians\n",
    "            or phi > self.phi_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(6,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 8.00 |\n",
      "| episode:  1  | score: 6.00 |\n",
      "| episode:  2  | score: 7.00 |\n",
      "| episode:  3  | score: 10.00 |\n",
      "| episode:  4  | score: 9.00 |\n",
      "| episode:  5  | score: 11.00 |\n",
      "| episode:  6  | score: 8.00 |\n",
      "| episode:  7  | score: 7.00 |\n",
      "| episode:  8  | score: 10.00 |\n",
      "| episode:  9  | score: 10.00 |\n",
      "| episode:  10  | score: 10.00 |\n",
      "| episode:  11  | score: 10.00 |\n",
      "| episode:  12  | score: 9.00 |\n",
      "| episode:  13  | score: 7.00 |\n",
      "| episode:  14  | score: 11.00 |\n",
      "| episode:  15  | score: 9.00 |\n",
      "| episode:  16  | score: 11.00 |\n",
      "| episode:  17  | score: 12.00 |\n",
      "| episode:  18  | score: 11.00 |\n",
      "| episode:  19  | score: 9.00 |\n",
      "| episode:  20  | score: 13.00 |\n",
      "| episode:  21  | score: 10.00 |\n",
      "| episode:  22  | score: 11.00 |\n",
      "| episode:  23  | score: 10.00 |\n",
      "| episode:  24  | score: 13.00 |\n",
      "| episode:  25  | score: 12.00 |\n",
      "| episode:  26  | score: 10.00 |\n",
      "| episode:  27  | score: 9.00 |\n",
      "| episode:  28  | score: 9.00 |\n",
      "| episode:  29  | score: 10.00 |\n",
      "| episode:  30  | score: 11.00 |\n",
      "| episode:  31  | score: 9.00 |\n",
      "| episode:  32  | score: 11.00 |\n",
      "| episode:  33  | score: 11.00 |\n",
      "| episode:  34  | score: 9.00 |\n",
      "| episode:  35  | score: 8.00 |\n",
      "| episode:  36  | score: 9.00 |\n",
      "| episode:  37  | score: 11.00 |\n",
      "| episode:  38  | score: 8.00 |\n",
      "| episode:  39  | score: 7.00 |\n",
      "| episode:  40  | score: 11.00 |\n",
      "| episode:  41  | score: 7.00 |\n",
      "| episode:  42  | score: 9.00 |\n",
      "| episode:  43  | score: 9.00 |\n",
      "| episode:  44  | score: 10.00 |\n",
      "| episode:  45  | score: 8.00 |\n",
      "| episode:  46  | score: 11.00 |\n",
      "| episode:  47  | score: 10.00 |\n",
      "| episode:  48  | score: 10.00 |\n",
      "| episode:  49  | score: 10.00 |\n",
      "| episode:  50  | score: 10.00 |\n",
      "| episode:  51  | score: 8.00 |\n",
      "| episode:  52  | score: 8.00 |\n",
      "| episode:  53  | score: 6.00 |\n",
      "| episode:  54  | score: 11.00 |\n",
      "| episode:  55  | score: 8.00 |\n",
      "| episode:  56  | score: 10.00 |\n",
      "| episode:  57  | score: 11.00 |\n",
      "| episode:  58  | score: 9.00 |\n",
      "| episode:  59  | score: 10.00 |\n",
      "| episode:  60  | score: 11.00 |\n",
      "| episode:  61  | score: 11.00 |\n",
      "| episode:  62  | score: 10.00 |\n",
      "| episode:  63  | score: 10.00 |\n",
      "| episode:  64  | score: 12.00 |\n",
      "| episode:  65  | score: 10.00 |\n",
      "| episode:  66  | score: 9.00 |\n",
      "| episode:  67  | score: 11.00 |\n",
      "| episode:  68  | score: 12.00 |\n",
      "| episode:  69  | score: 9.00 |\n",
      "| episode:  70  | score: 10.00 |\n",
      "| episode:  71  | score: 8.00 |\n",
      "| episode:  72  | score: 10.00 |\n",
      "| episode:  73  | score: 12.00 |\n",
      "| episode:  74  | score: 12.00 |\n",
      "| episode:  75  | score: 10.00 |\n",
      "| episode:  76  | score: 10.00 |\n",
      "| episode:  77  | score: 10.00 |\n",
      "| episode:  78  | score: 10.00 |\n",
      "| episode:  79  | score: 10.00 |\n",
      "| episode:  80  | score: 9.00 |\n",
      "| episode:  81  | score: 10.00 |\n",
      "| episode:  82  | score: 12.00 |\n",
      "| episode:  83  | score: 10.00 |\n",
      "| episode:  84  | score: 10.00 |\n",
      "| episode:  85  | score: 10.00 |\n",
      "| episode:  86  | score: 11.00 |\n",
      "| episode:  87  | score: 11.00 |\n",
      "| episode:  88  | score: 10.00 |\n",
      "| episode:  89  | score: 11.00 |\n",
      "| episode:  90  | score: 10.00 |\n",
      "| episode:  91  | score: 11.00 |\n",
      "| episode:  92  | score: 10.00 |\n",
      "| episode:  93  | score: 11.00 |\n",
      "| episode:  94  | score: 10.00 |\n",
      "| episode:  95  | score: 9.00 |\n",
      "| episode:  96  | score: 10.00 |\n",
      "| episode:  97  | score: 11.00 |\n",
      "| episode:  98  | score: 11.00 |\n",
      "| episode:  99  | score: 11.00 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = DoubleInvertedPendulumMA()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.2f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
