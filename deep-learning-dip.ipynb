{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "<u><h1 align=\"center\"> Deep Reinforcement Learning for Robotic Systems 🛠 </h1></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-childhood",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "**1. [Overview](#overview)<br />**\n",
    "\n",
    "**2. [Model](#model)<br />**\n",
    "&emsp;2.1. [Description](#model-description)<br />\n",
    "\n",
    "**3. [Governing Equations of Motion](#governing-eqs-motion)<br />**\n",
    "&emsp;3.1. [Library Imports](#library-imports)<br />\n",
    "&emsp;3.2. [Variable Declaration](#var-dec)<br />\n",
    "&emsp;3.3. [Kinetic and Potential Energy](#kinetic-potential)<br />\n",
    "&emsp;3.4. [The Lagrangian](#lagrangian)<br />\n",
    "&emsp;3.5. [The Euler-Lagrange Equations](#euler-lagrange)<br />\n",
    "&emsp;3.6. [Linearisation and Acceleration](#linearisation)<br />\n",
    "\n",
    "**4. [Proximal Policy Optimisation](#ppo)**<br />\n",
    "&emsp;4.1. [Overview](#ppo-overview)<br />\n",
    "&emsp;4.2. [Mathematical Model](#ppo-math)<br />\n",
    "&emsp;4.3. [Neural Network](#ppo-nn)<br />\n",
    "&emsp;&emsp;&emsp;*4.3.1. [Actor](#ppo-nn-actor)<br />*\n",
    "&emsp;&emsp;&emsp;*4.3.2. [Critic](#ppo-nn-critic)<br />*\n",
    "&emsp;&emsp;&emsp;*4.3.3. [Agent](#ppo-nn-agent)<br />*\n",
    "&emsp;4.4. [Environment](#ppo-env)<br />\n",
    "&emsp;4.5. [Test](#ppo-test)<br />\n",
    "\n",
    "**5. [Conclusion](#conclusion)**<br />\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview <a class=\"anchor\" id=\"overview\"></a>\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-olympus",
   "metadata": {},
   "source": [
    "## 2. Model <a class=\"anchor\" id=\"model\"></a>\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "### 2.1. Description <a class=\"anchor\" id=\"model-description\"></a>\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 3. Governing Equations of Motion <a class=\"anchor\" id=\"governing-eqs-motion\"></a> \n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-portable",
   "metadata": {},
   "source": [
    "### 3.1. Library Imports <a class=\"anchor\" id=\"library-imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gym\n",
    "import seeding\n",
    "import numpy as np\n",
    "import torch as T \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-brand",
   "metadata": {},
   "source": [
    "### 3.2. Variable Declaration <a class=\"anchor\" id=\"var-dec\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 3.3. Kinetic and Potential Energy <a class=\"anchor\" id=\"kinetic-potential\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 3.4. The Lagrangian <a class=\"anchor\" id=\"lagrangian\"></a>\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 3.5. The Euler-Lagrange Equations <a class=\"anchor\" id=\"euler-lagrange\"></a>\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 3.6. Linearisation and Acceleration <a class=\"anchor\" id=\"linearisation\"></a>\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 4. Proximal Policy Optimisation <a class=\"anchor\" id=\"ppo\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-chess",
   "metadata": {},
   "source": [
    "### 4.1. Overview[<sup>1</sup>](#fn1) <a class=\"anchor\" id=\"ppo-overview\"></a>\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-elizabeth",
   "metadata": {},
   "source": [
    "### 4.2. Mathematical Model <a class=\"anchor\" id=\"ppo-math\"></a>\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 4.3. Neural Network [A2C] <a class=\"anchor\" id=\"ppo-nn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-malta",
   "metadata": {},
   "source": [
    "#### 4.3.1. Actor <a class=\"anchor\" id=\"ppo-nn-actor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "julian-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-christopher",
   "metadata": {},
   "source": [
    "#### 4.3.2. Critic <a class=\"anchor\" id=\"ppo-nn-critic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "armed-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NOTE: See the above comments in the `ActorNetwork` for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-salon",
   "metadata": {},
   "source": [
    "#### 4.3.3. Agent <a class=\"anchor\" id=\"ppo-nn-agent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage class\n",
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 4.4. Environment <a class=\"anchor\" id=\"ppo-env\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from the classic cart-pole system implemented by Rich Sutton et al.\n",
    "\"\"\"\n",
    "\n",
    "class DoubleInvertedPendulumMA(gym.Env): \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.81\n",
    "        self.masscart = 1.0\n",
    "        self.masspole_1 = 0.1\n",
    "        self.masspole_2 = 0.1\n",
    "        self.masspole = (self.masspole_1 + self.masspole_2)\n",
    "        self.total_mass = (self.masscart + self.masspole)\n",
    "        self.lengthpole_1 = 0.25\n",
    "        self.lengthpole_2 = 0.25\n",
    "        self.length = (self.lengthpole_1 + self.lengthpole_2) \n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.phi_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        # distance of cart to fail episode\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.phi_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot, phi, phi_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        cosphi = math.cos(phi)\n",
    "        sinphi = math.cos(phi)\n",
    "        \n",
    "        # TODO: double-check the equations\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        \n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "\n",
    "        phiacc = (self.gravity * sinphi - sinphi * temp) / \\\n",
    "        (self.length * (4.0 / 3.0 - self.masspole * cosphi ** 2 / self.total_mass))\n",
    "        \n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi = phi + self.tau * phi_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            # print(f\"phi_dot: {phi_dot}, theta_dot{theta_dot}\")\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            phi_dot = phi_dot + self.tau * phiacc\n",
    "            phi = phi + self.tau * phi_dot  \n",
    "            \n",
    "        self.state = (x, x_dot, theta, theta_dot, phi, phi_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "            or phi < -self.phi_threshold_radians\n",
    "            or phi > self.phi_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(6,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 4.5. Test <a class=\"anchor\" id=\"ppo-test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 6 |\n",
      "| episode:  1  | score: 7 |\n",
      "| episode:  2  | score: 8 |\n",
      "| episode:  3  | score: 8 |\n",
      "| episode:  4  | score: 10 |\n",
      "| episode:  5  | score: 6 |\n",
      "| episode:  6  | score: 7 |\n",
      "| episode:  7  | score: 9 |\n",
      "| episode:  8  | score: 7 |\n",
      "| episode:  9  | score: 9 |\n",
      "| episode:  10  | score: 7 |\n",
      "| episode:  11  | score: 8 |\n",
      "| episode:  12  | score: 9 |\n",
      "| episode:  13  | score: 10 |\n",
      "| episode:  14  | score: 8 |\n",
      "| episode:  15  | score: 8 |\n",
      "| episode:  16  | score: 8 |\n",
      "| episode:  17  | score: 10 |\n",
      "| episode:  18  | score: 9 |\n",
      "| episode:  19  | score: 10 |\n",
      "| episode:  20  | score: 10 |\n",
      "| episode:  21  | score: 9 |\n",
      "| episode:  22  | score: 9 |\n",
      "| episode:  23  | score: 7 |\n",
      "| episode:  24  | score: 9 |\n",
      "| episode:  25  | score: 6 |\n",
      "| episode:  26  | score: 11 |\n",
      "| episode:  27  | score: 8 |\n",
      "| episode:  28  | score: 7 |\n",
      "| episode:  29  | score: 8 |\n",
      "| episode:  30  | score: 10 |\n",
      "| episode:  31  | score: 9 |\n",
      "| episode:  32  | score: 7 |\n",
      "| episode:  33  | score: 9 |\n",
      "| episode:  34  | score: 7 |\n",
      "| episode:  35  | score: 8 |\n",
      "| episode:  36  | score: 10 |\n",
      "| episode:  37  | score: 9 |\n",
      "| episode:  38  | score: 7 |\n",
      "| episode:  39  | score: 8 |\n",
      "| episode:  40  | score: 7 |\n",
      "| episode:  41  | score: 8 |\n",
      "| episode:  42  | score: 9 |\n",
      "| episode:  43  | score: 7 |\n",
      "| episode:  44  | score: 7 |\n",
      "| episode:  45  | score: 7 |\n",
      "| episode:  46  | score: 8 |\n",
      "| episode:  47  | score: 8 |\n",
      "| episode:  48  | score: 10 |\n",
      "| episode:  49  | score: 9 |\n",
      "| episode:  50  | score: 8 |\n",
      "| episode:  51  | score: 9 |\n",
      "| episode:  52  | score: 9 |\n",
      "| episode:  53  | score: 9 |\n",
      "| episode:  54  | score: 9 |\n",
      "| episode:  55  | score: 7 |\n",
      "| episode:  56  | score: 7 |\n",
      "| episode:  57  | score: 10 |\n",
      "| episode:  58  | score: 9 |\n",
      "| episode:  59  | score: 9 |\n",
      "| episode:  60  | score: 10 |\n",
      "| episode:  61  | score: 7 |\n",
      "| episode:  62  | score: 10 |\n",
      "| episode:  63  | score: 11 |\n",
      "| episode:  64  | score: 10 |\n",
      "| episode:  65  | score: 12 |\n",
      "| episode:  66  | score: 10 |\n",
      "| episode:  67  | score: 11 |\n",
      "| episode:  68  | score: 10 |\n",
      "| episode:  69  | score: 10 |\n",
      "| episode:  70  | score: 9 |\n",
      "| episode:  71  | score: 12 |\n",
      "| episode:  72  | score: 11 |\n",
      "| episode:  73  | score: 9 |\n",
      "| episode:  74  | score: 10 |\n",
      "| episode:  75  | score: 10 |\n",
      "| episode:  76  | score: 11 |\n",
      "| episode:  77  | score: 10 |\n",
      "| episode:  78  | score: 13 |\n",
      "| episode:  79  | score: 10 |\n",
      "| episode:  80  | score: 8 |\n",
      "| episode:  81  | score: 10 |\n",
      "| episode:  82  | score: 11 |\n",
      "| episode:  83  | score: 8 |\n",
      "| episode:  84  | score: 10 |\n",
      "| episode:  85  | score: 9 |\n",
      "| episode:  86  | score: 9 |\n",
      "| episode:  87  | score: 11 |\n",
      "| episode:  88  | score: 12 |\n",
      "| episode:  89  | score: 10 |\n",
      "| episode:  90  | score: 12 |\n",
      "| episode:  91  | score: 10 |\n",
      "| episode:  92  | score: 10 |\n",
      "| episode:  93  | score: 12 |\n",
      "| episode:  94  | score: 10 |\n",
      "| episode:  95  | score: 11 |\n",
      "| episode:  96  | score: 8 |\n",
      "| episode:  97  | score: 11 |\n",
      "| episode:  98  | score: 7 |\n",
      "| episode:  99  | score: 10 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = DoubleInvertedPendulumMA()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 3e-4\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.0f |' % score)\n",
    "    \n",
    "episodes = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(episode, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(episode, running_avg)\n",
    "    plt.title(f\"Learning curve for {num_games} games\", fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(episodes, score_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-jamaica",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "Apr. 2021 🤞🏽"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
