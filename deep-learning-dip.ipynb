{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "viral-score",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Robotic Systems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-announcement",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This notebook outlines the modelling and integration of the **[Proximal Policy Optimisation](http://arxiv.org/abs/1707.06347)** algorithm on an **inverted double pendulum** as a baseline study into advanced astrodynamical control systems, such as docking and berthing of spacecraft, and rocket trajectory stabilisation. \n",
    "\n",
    "--------\n",
    "\n",
    "Produced by *[Mughees Asif](https://github.com/mughees-asif)*, under the supervision of [Dr. Angadh Nanjangud](https://www.sems.qmul.ac.uk/staff/a.nanjangud) (Lecturer in Aerospace/Spacecraft Engineering @ [Queen Mary, University of London](https://www.sems.qmul.ac.uk/)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-latex",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "Proximal Policy Optimisation is a deep reinforcement learning algorithm developed by [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html). It has proven to be successful in a variety of tasks ranging from enabling robotic systems in complex environments, to developing proficiency in computer gaming by using stochastic mathematical modelling to simulate real-life decision making. For the purposes of this research, the algorithm will be implemented to vertically stablise an inverted double pendulum, which is widely used in industry as a benchmark to validate the veracity of next-generation intelligent algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-piece",
   "metadata": {},
   "source": [
    "## 2. Model description\n",
    "\n",
    "An inverted double pendulum is a characteristic example of a simple-to-build, non-linear, and chaotic mechanical system that has been widely studied in the fields of Robotics, Aerospace, Biomedical, Mechanical Engineering, and Mathematical Analysis.\n",
    "\n",
    "<img src=\"images/dip_fbd.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-upgrade",
   "metadata": {},
   "source": [
    "## 3. Variables\n",
    "\n",
    "<img src=\"images/variables.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-orleans",
   "metadata": {},
   "source": [
    "## 4. Governing equations of motion\n",
    "\n",
    "The following section utilises the [SymPy](https://www.sympy.org/en/index.html) package to derive the governing equations of motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-armstrong",
   "metadata": {},
   "source": [
    "### 4.1. Basic modelling\n",
    "\n",
    "<img src=\"images/dip_fbd_radius.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "preceding-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "# mathematical\n",
    "import sympy\n",
    "\n",
    "# computational\n",
    "import numpy as np\n",
    "import torch as T # PyTorch\n",
    "import torch.nn as nn # sequential model\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical # categorical distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import random as _random\n",
    "import struct\n",
    "import sys\n",
    "import math\n",
    "import gym\n",
    "import os\n",
    "import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "aerial-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliase variables\n",
    "t = sympy.symbols('t')        # time\n",
    "m = sympy.symbols('m')        # mass of the cart\n",
    "l = sympy.symbols('l')        # length of the pendulums, l_1 = l_2 = l\n",
    "M = sympy.symbols('M')        # mass of the pendulums, M_1 = M_2 = M\n",
    "I = sympy.symbols('I')        # moment of inertia\n",
    "g = sympy.symbols('g')        # gravitational constant, 9.81 m/s^2\n",
    "F = sympy.symbols('F')        # force applied to the cart\n",
    "\n",
    "x = sympy.Function('x')(t)    # |\n",
    "Θ = sympy.Function('Θ')(t)    # | --- functions of (t)\n",
    "Φ = sympy.Function('Φ')(t)    # |\n",
    "\n",
    "# cart\n",
    "x_dot = x.diff(t)             # velocity\n",
    "\n",
    "# pendulum(s) \n",
    "x_1 = x + (l*sympy.sin(Θ))    # | --- position\n",
    "x_2 = l*sympy.cos(Θ)          # | \n",
    "\n",
    "v_1 = x_1 + l*sympy.sin(Φ)                                             # |\n",
    "v_2 = x_2 + l*sympy.cos(Φ)                                             # | --- linear velocity\n",
    "v_3 = sympy.sqrt(sympy.simplify(x_1.diff(t)**2 + x_2.diff(t)**2))      # |  \n",
    "v_4 = sympy.sqrt(sympy.simplify(v_1.diff(t)**2 + v_2.diff(t)**2))      # |\n",
    "\n",
    "Θ_dot = Θ.diff(t)             # | --- angular velocity\n",
    "Φ_dot = Φ.diff(t)             # |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-clearance",
   "metadata": {},
   "source": [
    "### 4.2. Kinetic and Potential Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "figured-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinetic energy \n",
    "K = 0.5*((m*x_dot**2) + M*(v_3**2 + v_4**2) + I*(Θ_dot**2 + Φ_dot**2))\n",
    "\n",
    "# potential energy \n",
    "P = M*g*l*(2*sympy.cos(Θ) + sympy.cos(Φ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "possible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The kinetic energy, K, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe kinetic energy, K, of the system:\\n------------------------------')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "restricted-section",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The potential energy, P, of the system:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right)$"
      ],
      "text/plain": [
       "M*g*l*(2*cos(Θ(t)) + cos(Φ(t)))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nThe potential energy, P, of the system:\\n------------------------------')\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-soundtrack",
   "metadata": {},
   "source": [
    "### 4.3. The Lagrangian\n",
    "\n",
    "The action $S$ of the cart (movement; left, right) is mathematically defined as:\n",
    "\n",
    "$$S = \\int_{t_{0}}^{t_{1}} K - P \\,dt$$\n",
    "\n",
    "but, $L = K - P$\n",
    "\n",
    "$$\\therefore S = \\int_{t_{0}}^{t_{1}} L \\,dt$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "metallic-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Lagrangian of the system is:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.5 I \\left(\\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2}\\right) - M g l \\left(2 \\cos{\\left(Θ{\\left(t \\right)} \\right)} + \\cos{\\left(Φ{\\left(t \\right)} \\right)}\\right) + 0.5 M \\left(2 l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 l^{2} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + l^{2} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 4 l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Θ{\\left(t \\right)} + 2 l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d}{d t} x{\\left(t \\right)} \\frac{d}{d t} Φ{\\left(t \\right)} + 2 \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}\\right) + 0.5 m \\left(\\frac{d}{d t} x{\\left(t \\right)}\\right)^{2}$"
      ],
      "text/plain": [
       "0.5*I*(Derivative(Θ(t), t)**2 + Derivative(Φ(t), t)**2) - M*g*l*(2*cos(Θ(t)) + cos(Φ(t))) + 0.5*M*(2*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), t)*Derivative(Φ(t), t) + 2*l**2*Derivative(Θ(t), t)**2 + l**2*Derivative(Φ(t), t)**2 + 4*l*cos(Θ(t))*Derivative(x(t), t)*Derivative(Θ(t), t) + 2*l*cos(Φ(t))*Derivative(x(t), t)*Derivative(Φ(t), t) + 2*Derivative(x(t), t)**2) + 0.5*m*Derivative(x(t), t)**2"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the lagrangian\n",
    "L = K - P\n",
    "\n",
    "print('------------------------------\\nThe Lagrangian of the system is:\\n------------------------------')\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-percentage",
   "metadata": {},
   "source": [
    "### 4.4. The Euler-Lagrange equations\n",
    "\n",
    "The standard [Euler-Lagrange equation](https://www.ucl.ac.uk/~ucahmto/latex_html/chapter2_latex2html/node5.html) is:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = 0$$\n",
    "\n",
    "To introduce the generalised force acting on the cart, the [Lagrange-D'Alembert Principle](https://en.wikipedia.org/wiki/D%27Alembert%27s_principle) is used:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = Q^{P}$$\n",
    "\n",
    "Therefore, for a three-dimensional _working_ system, the equations of motion can be derived as:\n",
    "\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{x}} - \\frac{\\partial L}{\\partial x} = F - \\dot x$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\theta}} - \\frac{\\partial L}{\\partial \\theta} = 0$$\n",
    "$$\\frac{d}{dt}\\frac{\\partial L}{\\partial \\dot{\\phi}} - \\frac{\\partial L}{\\partial \\phi} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "broadband-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The Euler-Lagrange equations:\n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2 M l \\sin{\\left(Θ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} - M l \\sin{\\left(Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 2 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(-2*M*l*sin(Θ(t))*Derivative(Θ(t), t)**2 - M*l*sin(Φ(t))*Derivative(Φ(t), t)**2 + 2*M*l*cos(Θ(t))*Derivative(Θ(t), (t, 2)) + M*l*cos(Φ(t))*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# euler-lagrange formulation\n",
    "euler_1 = sympy.Eq((L.diff(x_dot).diff(t) - L.diff(x)).simplify().expand().collect(x.diff(t, t)), F - x.diff(t))\n",
    "euler_2 = sympy.Eq((L.diff(Θ_dot).diff(t) - L.diff(Θ)).simplify().expand().collect(Θ.diff(t, t)), 0)\n",
    "euler_3 = sympy.Eq((L.diff(Φ_dot).diff(t) - L.diff(Φ)).simplify().expand().collect(Φ.diff(t, t)), 0)\n",
    "\n",
    "print('------------------------------\\nThe Euler-Lagrange equations:\\n------------------------------\\n1.')\n",
    "euler_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "processed-membrane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l \\sin{\\left(Θ{\\left(t \\right)} \\right)} + 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Φ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\cos{\\left(Θ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*sin(Θ(t)) + 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Φ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Φ(t), (t, 2)) + 2.0*M*l*cos(Θ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2.')\n",
    "euler_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "circular-helicopter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l \\sin{\\left(Φ{\\left(t \\right)} \\right)} - 1.0 M l^{2} \\sin{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\left(\\frac{d}{d t} Θ{\\left(t \\right)}\\right)^{2} + 1.0 M l^{2} \\cos{\\left(Θ{\\left(t \\right)} - Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\cos{\\left(Φ{\\left(t \\right)} \\right)} \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*sin(Φ(t)) - 1.0*M*l**2*sin(Θ(t) - Φ(t))*Derivative(Θ(t), t)**2 + 1.0*M*l**2*cos(Θ(t) - Φ(t))*Derivative(Θ(t), (t, 2)) + 1.0*M*l*cos(Φ(t))*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3.')\n",
    "euler_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-congress",
   "metadata": {},
   "source": [
    "### 4.5. Linearisation and acceleration\n",
    "\n",
    "[Hartman-Grobman theorem](https://en.wikipedia.org/wiki/Hartman%E2%80%93Grobman_theorem)\n",
    "\n",
    "The pendulum will achieve equilibrium when vertical, i.e. $\\theta=0$ & $\\phi=0$:\n",
    "\n",
    "$$\\sin(\\theta)=\\theta, \\quad \\cos(\\theta)=1, \\quad \\dot\\theta^{2}=0$$\n",
    "\n",
    "$$\\sin(\\phi)=\\phi, \\quad \\cos(\\phi)=1, \\quad \\dot\\phi^{2}=0$$\n",
    "\n",
    "$$\\sin(\\theta - \\phi)=\\theta - \\phi, \\quad\\quad \\cos(\\theta - \\phi)=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "solid-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "The linear equations are: \n",
      "------------------------------\n",
      "1.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 M l \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + M l \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + \\left(2 M + 1.0 m\\right) \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} = F - \\frac{d}{d t} x{\\left(t \\right)}$"
      ],
      "text/plain": [
       "Eq(2*M*l*Derivative(Θ(t), (t, 2)) + M*l*Derivative(Φ(t), (t, 2)) + (2*M + 1.0*m)*Derivative(x(t), (t, 2)), F - Derivative(x(t), t))"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linearise the system\n",
    "matrix = [(sympy.sin(Θ), Θ), (sympy.cos(Θ), 1), (Θ_dot**2, 0), \n",
    "         (sympy.sin(Φ), Φ), (sympy.cos(Φ), 1), (Φ_dot**2, 0),\n",
    "         (sympy.sin(Θ - Φ), Θ - Φ), (sympy.cos(Θ - Φ), 1)]\n",
    "\n",
    "linear_1 = euler_1.subs(matrix)\n",
    "linear_2 = euler_2.subs(matrix)\n",
    "linear_3 = euler_3.subs(matrix)\n",
    "\n",
    "print('------------------------------\\nThe linear equations are: \\n------------------------------\\n1.')\n",
    "linear_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "painted-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 2.0 M g l Θ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} + 2.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 2.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-2.0*M*g*l*Θ(t) + 1.0*M*l**2*Derivative(Φ(t), (t, 2)) + 2.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 2.0*M*l**2)*Derivative(Θ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. ')\n",
    "linear_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "intellectual-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.0 M g l Φ{\\left(t \\right)} + 1.0 M l^{2} \\frac{d^{2}}{d t^{2}} Θ{\\left(t \\right)} + 1.0 M l \\frac{d^{2}}{d t^{2}} x{\\left(t \\right)} + \\left(1.0 I + 1.0 M l^{2}\\right) \\frac{d^{2}}{d t^{2}} Φ{\\left(t \\right)} = 0$"
      ],
      "text/plain": [
       "Eq(-1.0*M*g*l*Φ(t) + 1.0*M*l**2*Derivative(Θ(t), (t, 2)) + 1.0*M*l*Derivative(x(t), (t, 2)) + (1.0*I + 1.0*M*l**2)*Derivative(Φ(t), (t, 2)), 0)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. ')\n",
    "linear_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "statewide-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "x_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{F \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) - 4.0 I M^{2} g l^{2} Φ{\\left(t \\right)} - M^{2} g l^{2} \\left(16.0 I + 8.0 M l^{2}\\right) Θ{\\left(t \\right)} - \\left(4.0 I^{2} + 12.0 I M l^{2} + 4.0 M^{2} l^{4}\\right) \\frac{d}{d t} x{\\left(t \\right)}}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "(F*(4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4) - 4.0*I*M**2*g*l**2*Φ(t) - M**2*g*l**2*(16.0*I + 8.0*M*l**2)*Θ(t) - (4.0*I**2 + 12.0*I*M*l**2 + 4.0*M**2*l**4)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplify for linear and angular acceleration\n",
    "final_equations = sympy.linsolve([linear_1, linear_2, linear_3], [x.diff(t, t), Θ.diff(t, t), Φ.diff(t, t)])\n",
    "\n",
    "x_ddot = final_equations.args[0][0].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Θ_ddot = final_equations.args[0][1].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "Φ_ddot = final_equations.args[0][2].expand().collect((Θ, Θ_dot, x, x_dot, Φ, Φ_dot, F)).simplify()\n",
    "\n",
    "print('------------------------------\\nx_acceleration:\\n------------------------------')\n",
    "x_ddot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "sustainable-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Θ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- F \\left(8.0 I + 4.0 M l^{2}\\right) - 4.0 M g l^{2} m Φ{\\left(t \\right)} + g \\left(16.0 I M + 8.0 I m + 8.0 M^{2} l^{2} + 8.0 M l^{2} m\\right) Θ{\\left(t \\right)} + \\left(8.0 I + 4.0 M l^{2}\\right) \\frac{d}{d t} x{\\left(t \\right)}\\right)}{8.0 I^{2} M + 4.0 I^{2} m + 4.0 I M^{2} l^{2} + 12.0 I M l^{2} m + 4.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-F*(8.0*I + 4.0*M*l**2) - 4.0*M*g*l**2*m*Φ(t) + g*(16.0*I*M + 8.0*I*m + 8.0*M**2*l**2 + 8.0*M*l**2*m)*Θ(t) + (8.0*I + 4.0*M*l**2)*Derivative(x(t), t))/(8.0*I**2*M + 4.0*I**2*m + 4.0*I*M**2*l**2 + 12.0*I*M*l**2*m + 4.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΘ_acceleration:\\n------------------------------')\n",
    "Θ_ddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "sudden-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Φ_acceleration:\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{M l \\left(- 1.0 F I + 1.0 I \\frac{d}{d t} x{\\left(t \\right)} - 2.0 M g l^{2} m Θ{\\left(t \\right)} + g \\left(2.0 I M + 1.0 I m + 2.0 M l^{2} m\\right) Φ{\\left(t \\right)}\\right)}{2.0 I^{2} M + 1.0 I^{2} m + 1.0 I M^{2} l^{2} + 3.0 I M l^{2} m + 1.0 M^{2} l^{4} m}$"
      ],
      "text/plain": [
       "M*l*(-1.0*F*I + 1.0*I*Derivative(x(t), t) - 2.0*M*g*l**2*m*Θ(t) + g*(2.0*I*M + 1.0*I*m + 2.0*M*l**2*m)*Φ(t))/(2.0*I**2*M + 1.0*I**2*m + 1.0*I*M**2*l**2 + 3.0*I*M*l**2*m + 1.0*M**2*l**4*m)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------------------\\nΦ_acceleration:\\n------------------------------')\n",
    "Φ_ddot         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-turtle",
   "metadata": {},
   "source": [
    "## 5. Proximal Policy Optimisation\n",
    "\n",
    "### 5.1. Overview[<sup>1</sup>](#fn1)\n",
    " \n",
    " * State-of-the-art Policy Gradient method.\n",
    " * An on-policy algorithm.\n",
    " * Can be used for environments with either discrete or continuous action spaces.\n",
    " * **PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    " \n",
    "<sup>1</sup><span id=\"fn1\"></span>Referenced from [OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html) \n",
    "\n",
    "### 5.2. PPO-Clip mathematical model\n",
    "\n",
    "$$ \\begin{equation}\\mathbf{\n",
    " L^{PPO} (\\theta)=\\mathbb{\\hat{E}}_t\\:[L^{CLIP}(\\theta)-c_1L^{VF}(\\theta)+c_2S[\\pi_\\theta](s_t)]}\n",
    " \\end{equation}$$ \n",
    " \n",
    "1. $ L^{CLIP} (\\theta)=\\mathbb{\\hat{E}}_t[\\min(r_t(\\theta)\\:\\hat{A}^t,\\:\\:clip(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\hat{A}^t)]$ \n",
    "<br>*where*,\n",
    "* $r_t(\\theta)\\:\\hat{A}^t$: Surrogate objective is the probability ratio between a new policy network and an older policy network.\n",
    "\n",
    "* $\\epsilon$: Hyper-parameter; usually with a value of 0.2.\n",
    "\n",
    "* clip$(r_t(\\theta),\\:\\:1-\\epsilon,\\:\\:1+\\epsilon)\\:\\hat{A}^t$: Clipped version of the surrogate objective, where the probability ratio is truncated.\n",
    "\n",
    "2. $c_1L^{VF}(\\theta)$: Determines desirability of the current state.\n",
    "\n",
    "3. $c_2S[\\pi_\\theta](s_t)$: The entropy term using Gaussian Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-newfoundland",
   "metadata": {},
   "source": [
    "### 5.3. Neural Network [A2C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "dying-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOStorage:\n",
    "    # constructor - init values to empty lists\n",
    "    def __init__(self, batch_size):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # generate batches - defines the number of samples that will be propagated through the network\n",
    "    def generate_batches(self):\n",
    "        num_states = len(self.states_encountered)\n",
    "        batch_start = np.arange(0, num_states, self.batch_size)\n",
    "        idx = np.arange(num_states, dtype=np.int64)\n",
    "        np.random.shuffle(idx) # shuffle to handle stochastic gradient descent\n",
    "        batches = [idx[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        # NOTE: maintain return order\n",
    "        return np.array(self.states_encountered),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probability),\\\n",
    "                np.array(self.values),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.terminal_flag),\\\n",
    "                batches\n",
    "    \n",
    "    # store results from previous state\n",
    "    def memory_storage(self, states_encountered, action, probability, values, reward, terminal_flag):\n",
    "        self.states_encountered.append(states_encountered)\n",
    "        self.actions.append(action)\n",
    "        self.probability.append(probability)\n",
    "        self.values.append(values)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal_flag.append(terminal_flag)\n",
    "\n",
    "    # clear memory after retrieving state\n",
    "    def memory_clear(self):\n",
    "        self.states_encountered = []\n",
    "        self.probability = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminal_flag = []\n",
    "        self.values = []\n",
    "\n",
    "# defines the actor        \n",
    "class ActorNetwork(nn.Module):\n",
    "    # constructor\n",
    "    def __init__(self, num_actions, input_dimensions, learning_rate_alpha,\n",
    "            fully_connected_layer_1_dimensions=256, fully_connected_layer_2_dimensions=256, \n",
    "                 chkpt_dir='tmp/ppo'):\n",
    "        # call super-constructor \n",
    "        super(ActorNetwork, self).__init__()\n",
    "        # save checkpoint\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # deep neural network (DNN)\n",
    "        self.actor = nn.Sequential(\n",
    "                # linear layers unpack input_dimensions\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                # ReLU: applies the rectified linear unit function element-wise\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, num_actions),\n",
    "            \n",
    "                # softmax activation function: a mathematical function that converts a vector of numbers \n",
    "                # into a vector of probabilities, where the probabilities of each value are proportional to the \n",
    "                # relative scale of each value in the vector.\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # optimizer: an optimization algorithm that can be used instead of the classical stochastic \n",
    "        # gradient descent procedure to update network weights iterative based in training data\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        \n",
    "        # handle type of device\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # pass state forward through the DNN: calculate series of probabilities to draw from a distribution\n",
    "    # to get actual action. Use action to get log probabilities for the calculation of the two probablities\n",
    "    # for the learning function\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "\n",
    "# defines the critic [NOTE: See comments above for individual function explanation]          \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dimensions, learning_rate_alpha, fully_connected_layer_1_dimensions=256, \n",
    "                 fully_connected_layer_2_dimensions=256, chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        # self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dimensions, fully_connected_layer_1_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_1_dimensions, fully_connected_layer_2_dimensions),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fully_connected_layer_2_dimensions, 1)\n",
    "        )\n",
    "        \n",
    "        # same learning rate for both actor & critic -> actor is much more sensitive to the changes in the underlying\n",
    "        # parameters\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate_alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "# defines the agent \n",
    "class Agent:\n",
    "    def __init__(self, num_actions, input_dimensions, gamma=0.99, learning_rate_alpha=3e-4, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, num_epochs=10):\n",
    "        # save parameters\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(num_actions, input_dimensions, learning_rate_alpha)\n",
    "        self.critic = CriticNetwork(input_dimensions, learning_rate_alpha)\n",
    "        self.memory = PPOStorage(batch_size)\n",
    "    \n",
    "    # store memory; interface function\n",
    "    def interface_agent_memory(self, state, action, probability, values, reward, terminal_flag):\n",
    "        self.memory.memory_storage(state, action, probability, values, reward, terminal_flag)\n",
    "    \n",
    "    # choosing an action\n",
    "    def action_choice(self, observation):\n",
    "        # convert numpy array to a tensor\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        # distribution for choosing an action\n",
    "        dist = self.actor(state)\n",
    "        # value of the state\n",
    "        value = self.critic(state)\n",
    "        # sample distribution to get action\n",
    "        action = dist.sample()\n",
    "\n",
    "        # squeeze to eliminate batch dimensions\n",
    "        probability = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probability, value\n",
    "\n",
    "    # learning from actions\n",
    "    def learn(self):\n",
    "        # iterate over the number of epochs\n",
    "        for _ in range(self.num_epochs):\n",
    "            state_array, action_array, old_probability_array, values_array,\\\n",
    "            reward_array, terminal_flag_array, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = values_array\n",
    "            # advantage\n",
    "            advantage = np.zeros(len(reward_array), dtype=np.float32)\n",
    "            \n",
    "            # calculate advantage\n",
    "            for time_step in range(len(reward_array)-1):\n",
    "                discount = 1\n",
    "                advantage_time_step = 0\n",
    "                # from Schulman paper -> advantage function\n",
    "                for k in range(time_step, len(reward_array)-1):\n",
    "                    advantage_time_step += discount*(reward_array[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(terminal_flag_array[k])) - values[k])\n",
    "                    # multiplicative factor\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[time_step] = advantage_time_step\n",
    "            # turn advantage into tensor\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            # convert values to a tensor\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_array[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probability = T.tensor(old_probability_array[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_array[batch]).to(self.actor.device)\n",
    "                \n",
    "                # pi(theta)_new: take states and pass to Actor to get the new distribution for new probability\n",
    "                dist = self.actor(states)\n",
    "                \n",
    "                critic_value = self.critic(states)\n",
    "                # new values of the state according to the Critic network\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "                \n",
    "                # calculate new probability\n",
    "                new_probability = dist.log_prob(actions)\n",
    "                # probability ratio; probabilities taken as exponential to get ratio\n",
    "                probability_ratio = new_probability.exp() / old_probability.exp()\n",
    "                # prob_ratio = (new_probs - old_probs).exp()\n",
    "                \n",
    "                weighted_probability = advantage[batch] * probability_ratio\n",
    "                \n",
    "                weighted_clipped_probability = T.clamp(probability_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                \n",
    "                # negative due to gradient ascent\n",
    "                actor_loss = -T.min(weighted_probability, weighted_clipped_probability).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                \n",
    "                # zero the gradients\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                \n",
    "                # backpropagate total loss\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        \n",
    "        # at end of epochs clear memory\n",
    "        self.memory.memory_clear()               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-variety",
   "metadata": {},
   "source": [
    "### 5.5. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "surgical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array([self.x_threshold * 2,\n",
    "                         np.finfo(np.float32).max,\n",
    "                         self.theta_threshold_radians * 2,\n",
    "                         np.finfo(np.float32).max],\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-electron",
   "metadata": {},
   "source": [
    "### 5.6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "constant-friend",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| episode:  0  | score: 59.00 |\n",
      "| episode:  1  | score: 127.00 |\n",
      "| episode:  2  | score: 18.00 |\n",
      "| episode:  3  | score: 17.00 |\n",
      "| episode:  4  | score: 21.00 |\n",
      "| episode:  5  | score: 23.00 |\n",
      "| episode:  6  | score: 19.00 |\n",
      "| episode:  7  | score: 45.00 |\n",
      "| episode:  8  | score: 63.00 |\n",
      "| episode:  9  | score: 67.00 |\n",
      "| episode:  10  | score: 27.00 |\n",
      "| episode:  11  | score: 14.00 |\n",
      "| episode:  12  | score: 11.00 |\n",
      "| episode:  13  | score: 20.00 |\n",
      "| episode:  14  | score: 10.00 |\n",
      "| episode:  15  | score: 19.00 |\n",
      "| episode:  16  | score: 16.00 |\n",
      "| episode:  17  | score: 19.00 |\n",
      "| episode:  18  | score: 15.00 |\n",
      "| episode:  19  | score: 16.00 |\n",
      "| episode:  20  | score: 23.00 |\n",
      "| episode:  21  | score: 11.00 |\n",
      "| episode:  22  | score: 58.00 |\n",
      "| episode:  23  | score: 36.00 |\n",
      "| episode:  24  | score: 43.00 |\n",
      "| episode:  25  | score: 26.00 |\n",
      "| episode:  26  | score: 31.00 |\n",
      "| episode:  27  | score: 111.00 |\n",
      "| episode:  28  | score: 41.00 |\n",
      "| episode:  29  | score: 29.00 |\n",
      "| episode:  30  | score: 17.00 |\n",
      "| episode:  31  | score: 17.00 |\n",
      "| episode:  32  | score: 18.00 |\n",
      "| episode:  33  | score: 18.00 |\n",
      "| episode:  34  | score: 11.00 |\n",
      "| episode:  35  | score: 19.00 |\n",
      "| episode:  36  | score: 31.00 |\n",
      "| episode:  37  | score: 95.00 |\n",
      "| episode:  38  | score: 65.00 |\n",
      "| episode:  39  | score: 109.00 |\n",
      "| episode:  40  | score: 50.00 |\n",
      "| episode:  41  | score: 68.00 |\n",
      "| episode:  42  | score: 32.00 |\n",
      "| episode:  43  | score: 179.00 |\n",
      "| episode:  44  | score: 73.00 |\n",
      "| episode:  45  | score: 313.00 |\n",
      "| episode:  46  | score: 53.00 |\n",
      "| episode:  47  | score: 167.00 |\n",
      "| episode:  48  | score: 142.00 |\n",
      "| episode:  49  | score: 115.00 |\n",
      "| episode:  50  | score: 176.00 |\n",
      "| episode:  51  | score: 79.00 |\n",
      "| episode:  52  | score: 128.00 |\n",
      "| episode:  53  | score: 23.00 |\n",
      "| episode:  54  | score: 19.00 |\n",
      "| episode:  55  | score: 302.00 |\n",
      "| episode:  56  | score: 402.00 |\n",
      "| episode:  57  | score: 59.00 |\n",
      "| episode:  58  | score: 47.00 |\n",
      "| episode:  59  | score: 105.00 |\n",
      "| episode:  60  | score: 57.00 |\n",
      "| episode:  61  | score: 49.00 |\n",
      "| episode:  62  | score: 122.00 |\n",
      "| episode:  63  | score: 74.00 |\n",
      "| episode:  64  | score: 93.00 |\n",
      "| episode:  65  | score: 129.00 |\n",
      "| episode:  66  | score: 244.00 |\n",
      "| episode:  67  | score: 184.00 |\n",
      "| episode:  68  | score: 158.00 |\n",
      "| episode:  69  | score: 59.00 |\n",
      "| episode:  70  | score: 75.00 |\n",
      "| episode:  71  | score: 47.00 |\n",
      "| episode:  72  | score: 114.00 |\n",
      "| episode:  73  | score: 79.00 |\n",
      "| episode:  74  | score: 123.00 |\n",
      "| episode:  75  | score: 101.00 |\n",
      "| episode:  76  | score: 128.00 |\n",
      "| episode:  77  | score: 146.00 |\n",
      "| episode:  78  | score: 130.00 |\n",
      "| episode:  79  | score: 126.00 |\n",
      "| episode:  80  | score: 195.00 |\n",
      "| episode:  81  | score: 148.00 |\n",
      "| episode:  82  | score: 173.00 |\n",
      "| episode:  83  | score: 131.00 |\n",
      "| episode:  84  | score: 153.00 |\n",
      "| episode:  85  | score: 198.00 |\n",
      "| episode:  86  | score: 331.00 |\n",
      "| episode:  87  | score: 227.00 |\n",
      "| episode:  88  | score: 487.00 |\n",
      "| episode:  89  | score: 136.00 |\n",
      "| episode:  90  | score: 253.00 |\n",
      "| episode:  91  | score: 632.00 |\n",
      "| episode:  92  | score: 673.00 |\n",
      "| episode:  93  | score: 1431.00 |\n",
      "| episode:  94  | score: 1537.00 |\n",
      "| episode:  95  | score: 357.00 |\n",
      "| episode:  96  | score: 115.00 |\n",
      "| episode:  97  | score: 20.00 |\n",
      "| episode:  98  | score: 21.00 |\n",
      "| episode:  99  | score: 18.00 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create environment\n",
    "env = CartPoleEnv()\n",
    "N = 20\n",
    "batch_size = 5\n",
    "num_epochs = 4\n",
    "learning_rate_alpha = 0.0003\n",
    "agent = Agent(num_actions=env.action_space.n, batch_size=batch_size, \n",
    "                learning_rate_alpha=learning_rate_alpha, num_epochs=num_epochs, \n",
    "                input_dimensions=env.observation_space.shape)\n",
    "\n",
    "# number of games\n",
    "num_games = 100\n",
    "\n",
    "# track best score: minimum score for the environment\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "average_score = 0\n",
    "num_steps = 0\n",
    "\n",
    "for i in range(num_games):\n",
    "    observation = env.reset()\n",
    "    terminal_flag = False\n",
    "    score = 0\n",
    "    while not terminal_flag:\n",
    "        # choose action based on the current state of the environment\n",
    "        action, probability, value = agent.action_choice(observation)\n",
    "        observation_, reward, terminal_flag, info = env.step(action)\n",
    "        num_steps += 1\n",
    "        score += reward\n",
    "        \n",
    "        # store transition in the agent memory\n",
    "        agent.interface_agent_memory(observation, action, probability, value, reward, terminal_flag)\n",
    "        if num_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    average_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "\n",
    "    print('| episode: ', i, ' | score: %.2f |' % score)\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "\n",
    "def plot_learning_curve(x, scores):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Learning curve for %s games' % (x[-1]), fontweight='bold')\n",
    "    plt.xlabel('No. of games', fontsize=11)\n",
    "    plt.ylabel('Average reward', fontsize=11)\n",
    "    \n",
    "plot_learning_curve(x, score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
